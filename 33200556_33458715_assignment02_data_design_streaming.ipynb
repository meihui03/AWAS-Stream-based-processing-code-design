{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28accc5",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
    "---\n",
    "\n",
    "Teaching Team:\n",
    "\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
    "\n",
    "School of Information Technology, Monash University, Malaysia\n",
    "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
    "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
    "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
    "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
    "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af82432",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Group Information</span>\n",
    "---\n",
    "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
    "\n",
    "Your tutorial day and time: Thursday 6pm    <br/>\n",
    "\n",
    "1st group member\n",
    "\n",
    "Surname: Kang  <br/>\n",
    "Firstname: Jia Xin    <br/>\n",
    "Student ID: 33200556   <br/>\n",
    "Email: jkan0037@student.monash.edu   <br/>\n",
    "\n",
    "2nd group member\n",
    "\n",
    "Surname: Lok  <br/>\n",
    "Firstname: Mei Hui    <br/>\n",
    "Student ID: 33458715    <br/>\n",
    "Email: mlok0006@student.monash.edu    <br/>\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027af0",
   "metadata": {},
   "source": [
    "# Streaming Application\n",
    "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb43d2",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
    "\n",
    "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
    "\n",
    "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
    "\n",
    "This assignment contain **3 parts**:\n",
    "\n",
    "* **Part 1**: MongoDB Data Model (5 Marks)\n",
    "* **Part 2**: Streaming Application (20 Marks)\n",
    "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
    "* **Part 4**: Code demo and interview (Negative marking)\n",
    "\n",
    "Required Software:\n",
    "\n",
    "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
    "* Please use the provided Docker to load the Jupyter Notebook\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6830c",
   "metadata": {},
   "source": [
    "### Assignment Marking\n",
    "\n",
    "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652a7b4",
   "metadata": {},
   "source": [
    "### What to Submit\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
    "\n",
    "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
    "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
    "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
    "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
    "\n",
    "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
    "\n",
    "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f836d",
   "metadata": {},
   "source": [
    "### Plagiarism and Collusion\n",
    "\n",
    "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
    "\n",
    "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99713db4",
   "metadata": {},
   "source": [
    "### Generative AI usage\n",
    "\n",
    "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
    " \n",
    "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
    "You may use AI to help you learn how to solve the assignment.\n",
    "\n",
    "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
    " \n",
    "Any work submitted for a mark must:\n",
    "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
    "adhere to the guidelines for AI use set for the assessment task.\n",
    "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
    "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496b475",
   "metadata": {},
   "source": [
    "### Late submissions \n",
    "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384a47",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Preliminary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe6061",
   "metadata": {},
   "source": [
    "### Scenario Background\n",
    "\n",
    "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
    "\n",
    "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
    "\n",
    "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8e2b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 1 - Overview of AWAS</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9965391",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
    "\n",
    "#### vehicle.csv\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* owner_name (a string that contains name of the owner)\n",
    "* owner_addr (a string that contains the address of the owner)\n",
    "* vechicle_type (a string that represents the vechile model)\n",
    "* registration_date (date and time when the vehicle was registered)\n",
    "\n",
    "#### camera.csv\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* latitude (a float value representing latitude of camera)\n",
    "* longitude (a float value representing longitude of camera)\n",
    "* position (a float value tells at which kilometer point is the camera)\n",
    "* speed_limit (a float value of maximum legal speed for the segment)\n",
    "\n",
    "#### camera_event.csv\n",
    "* event_id (a string-based unique identifier to camera reading)\n",
    "* batch_id (a integer-based identifier to batch reading)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
    "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
    "\n",
    "#### camera_event_historic.csv\n",
    "* violation_id (a string-based unique identifier for violation record)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
    "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
    "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
    "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
    "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
    "\n",
    "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a9aab",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce3d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /opt/conda/lib/python3.8/site-packages (4.3.3)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /opt/conda/lib/python3.8/site-packages (from pymongo) (2.3.0)\n",
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.8/site-packages (2.2.10)\n"
     ]
    }
   ],
   "source": [
    "# Add pip statement if necessary\n",
    "!pip install pymongo\n",
    "!pip install kafka-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadfa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any imports here\n",
    "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from json import loads\n",
    "from pymongo import ReplaceOne\n",
    "from bson.json_util import loads\n",
    "from pymongo import UpdateOne, InsertOne, DeleteOne\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark import SparkContext # spark\n",
    "from pyspark.streaming import StreamingContext # spark streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b0dee",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
    "\n",
    "This section consists of 3 sub-questions\n",
    "\n",
    "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97125d",
   "metadata": {},
   "source": [
    "### Task 1.1 Collection Design\n",
    "\n",
    "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
    "* Vehicle (Store static metadata about each vehicle)\n",
    "* Camera (Store static definitions of each camera)\n",
    "* Violation (Records of flagged violations)\n",
    "\n",
    "For each collection, provide\n",
    "* 1-2 sentence description of why this collection exists\n",
    "* document schema and a sample document\n",
    "* indexes (if any) by specifying\n",
    "    * Fields (and sort order if applicable)\n",
    "    * Type\n",
    "    * Purpose of the index\n",
    "* shard key strategy (if any) by specifying\n",
    "    * Chosen shard key\n",
    "    * Shard key type\n",
    "    * Rationale\n",
    "* data retention policy (if applicable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bacd3c",
   "metadata": {},
   "source": [
    "---\n",
    "### Vehicle Collection\n",
    "\n",
    "**Purpose**  \n",
    "Stores static metadata about each vehicle, such as its plate number, type and owner details. This allows enrichment of streaming violation data with contextual vehicle information.\n",
    "\n",
    "**Schema:**\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"ObjectId\",\n",
    "  \"car_plate\": \"str\",\n",
    "  \"owner_name\": \"str\",\n",
    "  \"owner_addr\": \"str\",\n",
    "  \"registration_date\": \"datetime\",\n",
    "  \"vechicle_type\": \"str\"\n",
    "}\n",
    "```\n",
    "**Sample Document**\n",
    "{'_id': ObjectId('683696c10ccdfcf8fbd6aac6'), 'car_plate': 'FT 02', 'owner_addr': '943 Jalan Bukit Mawar, Kuala Lumpur', 'owner_name': 'Goh Mei Wei', 'registration_date': datetime.datetime(2006, 8, 22, 3, 18), 'vechicle_type': 'Coupe'}\n",
    "\n",
    "**Indexes**\n",
    "* Field:car_plate \n",
    "* Type: Single Field \n",
    "* Purpose: Unique lookup during violation enrichment and stream joins.\n",
    "\n",
    "**Shard Key Strategy**\n",
    "none\n",
    "\n",
    "**Data Retention Policy**\n",
    "none\n",
    "\n",
    "---\n",
    "### Camera Collection\n",
    "\n",
    "**Purpose**  \n",
    "Stores fixed metadata for each AWAS or AES camera. Used for calculating distance and applicable speed limits between two camera points.\n",
    "\n",
    "**Schema:**\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"ObjectId\",\n",
    "  \"camera_id\": \"float\",\n",
    "  \"latitude\": \"float\",\n",
    "  \"longitude\": \"float\",\n",
    "  \"position\": \"float\",\n",
    "  \"speed_limit\": \"float\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Sample Document**\n",
    "{'_id': ObjectId('683696c9f770ecd51494c0e7'), 'camera_id': 1.0, 'latitude': 2.157730731, 'longitude': 102.6601002, 'position': 152.5, 'speed_limit': 110.0}\n",
    "\n",
    "**Indexes**\n",
    "* Field: camera_id \n",
    "* Type: Single Field \n",
    "* Purpose: Fast lookup of camera details when joining with camera_event or calculating distance between cameras.\n",
    "\n",
    "**Shard Key Strategy**\n",
    "none\n",
    "\n",
    "**Data Retention Policy**\n",
    "none\n",
    "\n",
    "---\n",
    "### Violation Collection\n",
    "\n",
    "**Purpose**  \n",
    "Stores all violations identified by the AWAS system. Each document represents a completed point-to-point violation event.\n",
    "\n",
    "**Schema:**\n",
    "```json\n",
    "{\n",
    "  \"_id\": \"ObjectId\",\n",
    "  \"car_plate\": \"string\",\n",
    "  \"violation_date\": \"datetime\",\n",
    "  \"violation_record\":[\n",
    "    {\n",
    "      \"violation_id\": \"string\",             \n",
    "    \"camera_id_start\": \"int\",              \n",
    "    \"camera_id_end\": \"int\",  \n",
    "    \"timestamp_start\": \"datetime\",  \n",
    "    \"timestamp_end\": \"datetime\",   \n",
    "    \"speed_reading\": \"double\",          \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Sample Document**\n",
    "{'_id': ObjectId('6836b955d1ca29cc7dcdcfab'), 'car_plate': 'YE 6517', 'violation_date': datetime.datetime(2018, 11, 14, 0, 0), 'violation_records': [{'violation_id': '0ff38c74-7ee6-41cd-bc26-3a6060604a02', 'camera_id_start': 1, 'camera_id_end': 2, 'timestamp_start': datetime.datetime(2018, 11, 14, 8, 30, 11), 'timestamp_end': datetime.datetime(2018, 11, 14, 8, 30, 35), 'speed_reading': 145.0}]}\n",
    "\n",
    "\n",
    "**Indexes**\n",
    "* **Field:** `car_plate`  and `violation_date`\n",
    "{ car_plate: 1, timestamp_start: -1 }\n",
    "car plate in asceding, violation date in descending. \n",
    "* Type: compound index \n",
    "* **Purpose:** Efficiently retrieve recent violations for a specific car.\n",
    "\n",
    "**Shard Key Strategy**\n",
    "none\n",
    "\n",
    "**Data Retention Policy**\n",
    "none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0092f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to local MongoDB instance\n",
    "\n",
    "# host_ip = \"192.168.68.124\"\n",
    "# host_ip = \"192.168.100.165\"\n",
    "host_ip = \"10.192.0.245\"\n",
    "# 10.192.0.245\n",
    "\n",
    "client = MongoClient(\n",
    "    host=f'{host_ip}',\n",
    "    port=27017\n",
    ")\n",
    "\n",
    "# Create the database\n",
    "db = client[\"traffic_monitoring\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87bef615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vehicle = pd.read_csv(\"data/vehicle.csv\")\n",
    "df_camera = pd.read_csv(\"data/camera.csv\")\n",
    "df_violation = pd.read_csv(\"data/camera_event_historic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f61c221f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  car_plate           owner_name                              owner_addr  \\\n",
      "0     FT 02          Goh Mei Wei     943 Jalan Bukit Mawar, Kuala Lumpur   \n",
      "1   DQZ 793  Muhammad bin Liyana  946 Jalan Bukit Jelutong, Kuala Lumpur   \n",
      "2   GUT 393      Haziq bin Azlan   900 Jalan Bukit Bintang, Kuala Lumpur   \n",
      "\n",
      "  vechicle_type    registration_date  \n",
      "0         Coupe  2006-08-22T03:18:00  \n",
      "1           SUV  2008-04-13T09:01:23  \n",
      "2         Sedan  2008-01-20T20:50:45  \n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(df_vehicle.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d5dec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               car_plate\n",
      "Unique Plates       9844\n",
      "Total Records      10000\n"
     ]
    }
   ],
   "source": [
    "summary = df_vehicle.agg({\n",
    "    \"car_plate\": [\"nunique\", \"count\"]\n",
    "})\n",
    "\n",
    "print(summary.rename(index={\"nunique\": \"Unique Plates\", \"count\": \"Total Records\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c0c8832",
   "metadata": {},
   "outputs": [],
   "source": [
    "db[\"vehicle\"].drop() # same as db.vehicle.drop()\n",
    "db[\"camera\"].drop()\n",
    "db[\"violation\"].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c29cf85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle index \n",
    "db[\"vehicle\"].create_index(\"car_plate\", unique=True) # ascending by default \n",
    "\n",
    "bulk_ops = []\n",
    "\n",
    "for _, row in df_vehicle.iterrows():\n",
    "    record = loads(row.to_json())\n",
    "    reg_date = pd.to_datetime(record.pop(\"registration_date\")).to_pydatetime()  # remove from record\n",
    "    \n",
    "    bulk_ops.append(UpdateOne(\n",
    "        {\"car_plate\": record[\"car_plate\"]},\n",
    "        {\n",
    "            \"$setOnInsert\": record,  # everything except registration_date\n",
    "            \"$max\": {\"registration_date\": reg_date}\n",
    "        },\n",
    "        upsert=True\n",
    "    ))\n",
    "\n",
    "# Execute all updates in bulk\n",
    "if bulk_ops:\n",
    "    db.vehicle.bulk_write(bulk_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acd4b079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('683696c10ccdfcf8fbd6aac6'), 'car_plate': 'FT 02', 'owner_addr': '943 Jalan Bukit Mawar, Kuala Lumpur', 'owner_name': 'Goh Mei Wei', 'registration_date': datetime.datetime(2006, 8, 22, 3, 18), 'vechicle_type': 'Coupe'}\n",
      "{'_id': ObjectId('683696c10ccdfcf8fbd6aac7'), 'car_plate': 'DQZ 793', 'owner_addr': '946 Jalan Bukit Jelutong, Kuala Lumpur', 'owner_name': 'Muhammad bin Liyana', 'registration_date': datetime.datetime(2008, 4, 13, 9, 1, 23), 'vechicle_type': 'SUV'}\n",
      "{'_id': ObjectId('683696c10ccdfcf8fbd6aac8'), 'car_plate': 'GUT 393', 'owner_addr': '900 Jalan Bukit Bintang, Kuala Lumpur', 'owner_name': 'Haziq bin Azlan', 'registration_date': datetime.datetime(2008, 1, 20, 20, 50, 45), 'vechicle_type': 'Sedan'}\n"
     ]
    }
   ],
   "source": [
    "for doc in db[\"vehicle\"].find({}).limit(3):\n",
    "    print(doc)\n",
    "# index created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b267e42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   camera_id  latitude   longitude  position  speed_limit\n",
      "0          1  2.157731  102.660100     152.5          110\n",
      "1          2  2.162419  102.652455     153.5          110\n",
      "2          3  2.167353  102.644914     154.5           90\n"
     ]
    }
   ],
   "source": [
    "# checking \n",
    "print(df_camera.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e261550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inserted: 3 records\n"
     ]
    }
   ],
   "source": [
    "# camera index \n",
    "db[\"camera\"].create_index(\"camera_id\", unique=True)\n",
    "# Prepare records\n",
    "df_camera[\"camera_id\"] = df_camera[\"camera_id\"].astype(int)\n",
    "df_camera[\"speed_limit\"] = df_camera[\"speed_limit\"].astype(int)\n",
    "\n",
    "# Convert DataFrame rows to dicts for MongoDB\n",
    "records = [loads(row.to_json()) for _, row in df_camera.iterrows()]\n",
    "\n",
    "bulk_ops = [InsertOne(record) for record in records]\n",
    "\n",
    "# Execute inserts\n",
    "try:\n",
    "    if bulk_ops:\n",
    "        result = db.camera.bulk_write(bulk_ops, ordered=False)\n",
    "        print(f\" Inserted: {result.inserted_count} records\")\n",
    "except Exception as e:\n",
    "    print(\" Insert error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bb275a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('683696c9f770ecd51494c0e7'), 'camera_id': 1.0, 'latitude': 2.157730731, 'longitude': 102.6601002, 'position': 152.5, 'speed_limit': 110.0}\n",
      "{'_id': ObjectId('683696c9f770ecd51494c0e8'), 'camera_id': 2.0, 'latitude': 2.162418757, 'longitude': 102.6524549, 'position': 153.5, 'speed_limit': 110.0}\n",
      "{'_id': ObjectId('683696c9f770ecd51494c0e9'), 'camera_id': 3.0, 'latitude': 2.167352891, 'longitude': 102.6449144, 'position': 154.5, 'speed_limit': 90.0}\n"
     ]
    }
   ],
   "source": [
    "for doc in db[\"camera\"].find({}).limit(3):\n",
    "    print(doc)\n",
    "# index created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff7e59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           violation_id car_plate  camera_id_start  \\\n",
      "0  0ff38c74-7ee6-41cd-bc26-3a6060604a02   YE 6517                1   \n",
      "1  2cf78fb9-bea2-4dd8-951a-0476057e9710     AN 73                2   \n",
      "2  36136182-2be9-44d7-bebc-0a9b4d56b846      HC 0                1   \n",
      "\n",
      "   camera_id_end             timestamp_start               timestamp_end  \\\n",
      "0              2         2018-11-14T08:30:11  2018-11-14T08:30:35.821118   \n",
      "1              3  2018-08-30T08:29:18.936540  2018-08-30T08:29:51.037375   \n",
      "2              2         2018-10-27T08:34:34  2018-10-27T08:35:04.346445   \n",
      "\n",
      "   speed_reading  \n",
      "0          145.0  \n",
      "1          112.1  \n",
      "2          118.6  \n"
     ]
    }
   ],
   "source": [
    "#checking\n",
    "print(df_violation.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59ff58db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id_': {'v': 2, 'key': [('_id', 1)]}, 'car_plate_1_violation_date_-1': {'v': 2, 'key': [('car_plate', 1), ('violation_date', -1)], 'unique': True}}\n"
     ]
    }
   ],
   "source": [
    "db[\"violation\"].create_index(\n",
    "    [(\"car_plate\", 1), (\"violation_date\", -1)],\n",
    "    unique=True\n",
    ")\n",
    "print(db[\"violation\"].index_information())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11b33ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               violation_id car_plate  camera_id_start  \\\n",
      "15391  4dbadeef-ef4b-4c68-9914-edc9faab40f5     PQY 9                2   \n",
      "18027  b35aea26-538c-4599-88a1-8238a29af3cb     PQY 9                1   \n",
      "23958  7e01ad0b-00b7-4ee3-9176-70caabb9e201     PQY 9                1   \n",
      "29300  361dfca2-d271-480a-ae9c-925afbe50fd5     PQY 9                1   \n",
      "38423  28b2a3d5-92b8-43ff-9e21-ea65c1a97226     PQY 9                2   \n",
      "47918  848c848c-bdaf-43f9-9425-ff84da5200ac     PQY 9                2   \n",
      "\n",
      "       camera_id_end             timestamp_start               timestamp_end  \\\n",
      "15391              3  2015-05-13T08:26:05.404447  2015-05-13T08:26:38.529249   \n",
      "18027              2         2020-05-04T08:09:57  2020-05-04T08:10:21.156197   \n",
      "23958              2         2018-12-09T08:18:37  2018-12-09T08:19:06.646131   \n",
      "29300              2         2020-08-31T08:01:00  2020-08-31T08:01:29.422407   \n",
      "38423              3  2018-10-11T08:32:46.902422  2018-10-11T08:33:24.141596   \n",
      "47918              3  2015-05-12T08:28:19.502184  2015-05-12T08:28:52.607866   \n",
      "\n",
      "       speed_reading  \n",
      "15391          108.7  \n",
      "18027          149.0  \n",
      "23958          121.4  \n",
      "29300          122.4  \n",
      "38423           96.7  \n",
      "47918          108.7  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_df = df_violation[df_violation[\"car_plate\"] == \"PQY 9\"]\n",
    "print(result_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b23a0397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Inserted: 50000\n",
      "Skipped: 0\n",
      "Total documents in DB: 50000\n"
     ]
    }
   ],
   "source": [
    "inserted_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for _, row in df_violation.iterrows():\n",
    "    d_json = loads(row.to_json())\n",
    "\n",
    "    car_plate = d_json[\"car_plate\"].strip()\n",
    "    timestamp_end = pd.to_datetime(d_json[\"timestamp_end\"]).replace(microsecond=0, tzinfo=None)\n",
    "    timestamp_start = pd.to_datetime(d_json[\"timestamp_start\"]).replace(microsecond=0, tzinfo=None)\n",
    "    violation_date = datetime.combine(timestamp_end.date(), datetime.min.time())\n",
    "\n",
    "    query = {\"car_plate\": car_plate, \"violation_date\": violation_date}\n",
    "\n",
    "    violation_record = {\n",
    "        \"violation_id\": str(d_json[\"violation_id\"]),\n",
    "        \"camera_id_start\": int(d_json[\"camera_id_start\"]),\n",
    "        \"camera_id_end\": int(d_json[\"camera_id_end\"]),\n",
    "        \"timestamp_start\": timestamp_start,\n",
    "        \"timestamp_end\": timestamp_end,\n",
    "        \"speed_reading\": d_json[\"speed_reading\"]\n",
    "    }\n",
    "\n",
    "    existing = db.violation.find_one(query)\n",
    "\n",
    "    if existing is None:\n",
    "        db.violation.insert_one({\n",
    "            \"car_plate\": car_plate,\n",
    "            \"violation_date\": violation_date,\n",
    "            \"violation_records\": [violation_record]\n",
    "        })\n",
    "        inserted_count += 1\n",
    "    else:\n",
    "        if not any(\n",
    "            r[\"timestamp_start\"] == timestamp_start and r[\"timestamp_end\"] == timestamp_end\n",
    "            for r in existing[\"violation_records\"]\n",
    "        ):\n",
    "            db.violation.update_one(\n",
    "                {\"_id\": existing[\"_id\"]},\n",
    "                {\"$push\": {\"violation_records\": violation_record}}\n",
    "            )\n",
    "        skipped_count += 1\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Inserted:\", inserted_count)\n",
    "print(\"Skipped:\", skipped_count)\n",
    "print(\"Total documents in DB:\", db.violation.count_documents({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db5c3",
   "metadata": {},
   "source": [
    "### Task 1.2 Collection Relationship\n",
    "\n",
    "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
    "* Read/write patterns\n",
    "* Data duplication versus Join cost\n",
    "* Consistency requirements (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0006f42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### 1. Relationship: `violation.car_plate` → `vehicle.car_plates` (one to one because one car plate maps to one vehicle record.)\n",
    "\n",
    "**Design Choice:** Store `car_plate` as a reference.\n",
    "\n",
    "**Justification:**\n",
    "- **Read/Write Pattern:** Typically, there is no direct read or write interaction between the Violation and Vehicle collections; the relationship exists solely as a reference.\n",
    "- **Data Duplication vs Join Cost:** Storing a reference avoids duplication and reduces update propagation issues.\n",
    "- **Consistency:** Referencing ensures data remains consistent even if vehicle details are updated separately.\n",
    "\n",
    "---\n",
    "   \n",
    "\n",
    "#### 2. Relationship: `violation.violation_records[].camera_id_start and camera_id_end` → `camera.camera_id` (many to one, start camera and end camera may be the same camera)\n",
    "\n",
    "**Design Choice:** Store `camera_id_start` and `camera_id_end` as a reference.\n",
    "\n",
    "\n",
    "**Justification:**\n",
    "- **Read/Write Pattern:** Typically, there is no direct read or write interaction between the violation and camera collections; the relationship exists solely as a reference.\n",
    "- **Data Duplication vs Join Cost:** Camera data (e.g., geolocation, speed limit) is small but shared across many records. Referencing is more efficient and scalable.\n",
    "- **Consistency:** Cameras are rarely updated. Referencing ensures data remains consistent even if vehicle details are updated separately.\n",
    "\n",
    "---\n",
    "   \n",
    "\n",
    "#### 3. Relationship: `violation.violation_records[]` embedded in `violation ` (one to many, Each violation entry, grouped by date and car plate, contains one or more associated violation records.)\n",
    "\n",
    "**Design Choice:** Embed the violation records within the violation document identified by the car plate and violation date.\n",
    "\n",
    "**Justification:**\n",
    "- **Read/Write Pattern:** Embedding is suitable because violations are typically written and queried in batches, grouped by car plate and date.\n",
    "- **Data Duplication vs Join Cost:** The script inserts or updates daily violation documents per car, embedding unique records while preventing duplicates based on timestamps.\n",
    "- **Consistency:** All violations for a specific car on a given day are stored in a single document, containing one or more unique violation records for that day.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797ff17",
   "metadata": {},
   "source": [
    "### Task 1.3 Discussion\n",
    "\n",
    "In this task, discuss whether your model supports\n",
    "* Consistency and Idempotency\n",
    "    * Does it support idempotent writes?\n",
    "    * Explain any upsert pattern in `violation` collection\n",
    "* Scalability and Fault-Tolerance\n",
    "    * Can your data model support high ingest rates?\n",
    "    * Can your data model support low-latency lookups?\n",
    "\n",
    "Justify and explain the trade-off made in your design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46aeed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Consistency and Idempotency\n",
    "\n",
    "### Idempotent Writes\n",
    "Yes, the design supports idempotent writes. \n",
    "Idempotency is ensured by generating a unique violation_id via hashing car_plate, timestamp_start, and timestamp_end, and using $addToSet to prevent duplicates. update_one with upsert=True maintains one document per car per day, creating or updating as needed to group violations by date.\n",
    "\n",
    "### Upsert Pattern in Violation Collection\n",
    "In cases where a violation may need to be updated (e.g., speed re-evaluation or late-arriving timestamps), the system can perform an upsert operation.\n",
    "\n",
    "Each violation record is uniquely defined by key fields and a deterministic violation_id, hashed from car_plate, timestamp_start, and timestamp_end. Using $addToSet prevents duplicates, while upsert=True ensures the document is created if absent or updated safely if it exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8bfe88",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Scalability and Fault-Tolerance\n",
    "\n",
    "### Support for High Ingest Rates\n",
    "Yes, the data model is optimized for high write throughput. The model ingest high-throughput data via Apache Kafka and process it using Spark Structured Streaming with watermarks for event-time handling and late data.Data is processed in 5-second micro-batches to balance latency and throughput. For efficient writes, we use MongoDB’s bulk_write, reducing database round-trips and improving performance.\n",
    "\n",
    "### Support for Low-Latency Lookups\n",
    "Yes, query performance is optimized through indexing and referencing.\n",
    "A compound index on car_plate and violation_date enables fast violation lookups, while a single-field index on camera_id speeds up camera detail queries. Embedding daily violations per car in one document further optimizes retrieval by eliminating joins.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fffd5c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Trade-offs and Justification\n",
    "\n",
    "Vehicle and camera data are referenced rather than embedded, allowing global updates (e.g., speed limit changes) at the cost of additional lookups—mitigated by proper indexing. For writing, we use MongoDB’s bulk_write during the DbWriter.close() phase to reduce round-trips, trading off some local memory overhead, which is managed by limiting micro-batch intervals to 5 seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafe9a9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
    "\n",
    "This section consists of 2 sub-questions. \n",
    "\n",
    "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51a153",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"> \n",
    "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fc3a8",
   "metadata": {},
   "source": [
    "### Task 2.1 Data Stream Processing\n",
    "\n",
    "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
    "\n",
    "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989065f",
   "metadata": {},
   "source": [
    "#### Event Producer\n",
    "\n",
    "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7a85d",
   "metadata": {},
   "source": [
    "#### Streaming Application\n",
    "\n",
    "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
    "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
    "* If there is a violation detected, store it into MongoDB straight away.\n",
    "* If there is no violation detected, drop the record.\n",
    "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
    "\n",
    "##### Violation Detection Rule\n",
    "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
    "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
    "* Average speed of vehicle exceed the speed limit of the ending camera\n",
    "\n",
    "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d712ea",
   "metadata": {},
   "source": [
    "## Producers\n",
    "\n",
    "Each producer simulates a camera stream by sending batches of vehicle events from its respective CSV file. The producers operate on different batch intervals to emulate real-world asynchronous camera behavior.\n",
    "\n",
    "- **Camera A Producer**: Sends a new batch every **10 seconds**\n",
    "- **Camera B Producer**: Sends a new batch every **1 second**\n",
    "- **Camera C Producer**: Sends a new batch every **1 second**\n",
    "\n",
    "These producers publish events to the Kafka topics:\n",
    "- `camera_event`\n",
    "\n",
    "Explanation for time internal: \n",
    "internals of timestamps between batches are different across diff producer, thus we need to set different inetrval to acommodate for this, so that the batches will line up more closely, the watermark used in spark streaming will not eliminate certain data before it is able to join. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca3a5b3",
   "metadata": {},
   "source": [
    "## Stream Preprocessing\n",
    "\n",
    "Each stream is assigned an alias and its column names are **renamed with numeric suffixes** (`_1`, `_2`, `_3`) to:\n",
    "- Ensure clarity during joins\n",
    "- Prevent column name conflicts\n",
    "- Enable better traceability in logs and output\n",
    "\n",
    "For example, `event_time` from Camera A becomes `event_time_1`, and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca7673",
   "metadata": {},
   "source": [
    "## Speed Violation Detection Logic\n",
    "\n",
    "We detect two types of violations:\n",
    "\n",
    "#### 1. **Single-Point Violations (A,B,C)**  \n",
    "Each stream is individually filtered to detect cases where a vehicle's `speed_reading` exceeds the legal threshold. These are simple violations where only one camera's data is used.\n",
    "After detecting individual speed violations from **Camera A**, **Camera B**, and **Camera C**, each stream is standardized into a common format using a helper function. This ensures all streams share the same schema, with fields such as:\n",
    "\n",
    "- `producer_id`\n",
    "- `event_time`\n",
    "- `event_id`\n",
    "- `batch_id`\n",
    "- `car_plate`\n",
    "- `speed_reading`\n",
    "\n",
    "we then **combine all single-point violations into one unified stream** using PySpark’s `union()` operation\n",
    "\n",
    "---\n",
    "#### 2. **Join-Based Violations (AB & BC)**  \n",
    "We perform inner joins between:\n",
    "- **Camera A & B**: (`AB_violation`)\n",
    "- **Camera B & C**: (`BC_violation`)\n",
    "\n",
    "**Join conditions:**\n",
    "- Matching `car_plate`\n",
    "- The second camera’s event time occurs *after* the first\n",
    "- The time difference is *within 1 hour*\n",
    "\n",
    "We calculate the **average speed** between the two cameras and flag it as a violation if it exceeds the limit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc806b1",
   "metadata": {},
   "source": [
    "## Stream-to-MongoDB Writers\n",
    "\n",
    "There are three separate **Spark Structured Streaming writers**, each responsible for writing a different type of violation to MongoDB in real-time:\n",
    "\n",
    "1. **AB Join Violation Writer**\n",
    "   - Handles violations detected between Camera A and Camera B\n",
    "   - Uses average speed between two event times and two camera locations\n",
    "   - Calls `write_union_batch_to_mongo(df, batch_id, 1)` with violation type `1`\n",
    "\n",
    "2. **BC Join Violation Writer**\n",
    "   - Handles violations between Camera B and Camera C using the same join-based logic\n",
    "   - Also uses violation type `1` since it’s a multi-camera event\n",
    "\n",
    "3. **Single-Point Violation Writer**\n",
    "   - Handles individual violations from Camera A, B, or C independently\n",
    "   - Combines all single-point violations into one stream via `violations_union`\n",
    "   - Uses violation type `2` to indicate it's a single-camera event\n",
    "\n",
    "Each writer uses `foreachBatch()` to process micro-batches and write them to MongoDB with **exactly-once semantics**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "add66ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, unix_timestamp, expr, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "30b011ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section configures and initializes the Spark environment for reading Kafka streams.\n",
    "It sets required packages, creates a local Spark session and adjusts time parsing settings for legacy compatibility.\n",
    "\"\"\"\n",
    "\n",
    "# Set up Spark environment for Kafka\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Traffic Violation Detector\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7641becc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section sets the Kafka broker IP and topic name, then defines the schema used \n",
    "to parse the JSON messages from Kafka into structured Spark DataFrames.\n",
    "\"\"\"\n",
    "\n",
    "# Kafka configuration\n",
    "# host_ip = \"10.192.1.110\"\n",
    "# host_ip = \"192.168.100.165\"\n",
    "host_ip = \"10.192.0.245\"\n",
    "topic = \"camera_events\"\n",
    "\n",
    "# Define schema for Kafka values\n",
    "record = StructType([\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"producer_id\", IntegerType()),\n",
    "    StructField(\"event_id\", StringType()),\n",
    "    StructField(\"batch_id\", IntegerType()),\n",
    "    StructField(\"camera_id\", IntegerType()),\n",
    "    StructField(\"car_plate\", StringType()),\n",
    "    StructField(\"speed_reading\", DoubleType())\n",
    "])\n",
    "\n",
    "array = ArrayType(record)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a55dc81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section reads streaming data from the configured Kafka topic, \n",
    "parses the nested JSON structure and extracts relevant fields into a structured DataFrame.\n",
    "\"\"\"\n",
    "# Read Kafka stream\n",
    "kafka_s = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()\n",
    "\n",
    "# Parse Kafka messages\n",
    "parsed_s = kafka_s \\\n",
    "    .withColumn(\"json_str\", col(\"value\").cast(\"string\")) \\\n",
    "    .withColumn(\"records\", from_json(col(\"json_str\"), array)) \\\n",
    "    .withColumn(\"record\", explode(col(\"records\"))) \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"record.timestamp\"), \"yyyy-MM-dd'T'HH:mm:ss\")) \\\n",
    "    .select(\n",
    "        col(\"event_time\"),\n",
    "        col(\"record.producer_id\"),\n",
    "        col(\"record.event_id\"),\n",
    "        col(\"record.batch_id\"),\n",
    "        col(\"record.camera_id\"),\n",
    "        col(\"record.car_plate\"),\n",
    "        col(\"record.speed_reading\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f980e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_batch(df, batch_id):\n",
    "    print(f\"\\nBatch {batch_id} received at {datetime.utcnow().isoformat()}\")\n",
    "    count = df.count()\n",
    "    if count == 0:\n",
    "        print(\"Empty batch.\")\n",
    "    else:\n",
    "        print(f\"Rows in batch: {count}\")\n",
    "        df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e4f4b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section connects to the MongoDB server and retrieves camera configuration details \n",
    ", like speed limits from the 'camera' collection in the 'traffic_monitoring' database.\n",
    "The result is stored in a dictionary keyed by camera_id for quick access during stream processing.\n",
    "\"\"\"\n",
    "\n",
    "# MongoDB configuration and fetch camera info\n",
    "mongo_client = MongoClient(host=host_ip, port=27017)\n",
    "cam_s = {cam[\"camera_id\"]: cam for cam in mongo_client[\"traffic_monitoring\"][\"camera\"].find()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "dbc46107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section splits the parsed stream into three separate streams—one for each camera—\n",
    "based on the `producer_id` field. A watermark of 90 minutes is applied to each stream to manage \n",
    "late data and enable proper event-time-based operations like joins and windowing.\n",
    "\"\"\"\n",
    "\n",
    "# Define Streams per producer_id \n",
    "stream_A = parsed_s.filter(col(\"producer_id\") == \"1\").withWatermark(\"event_time\", \"90 minutes\")\n",
    "stream_B = parsed_s.filter(col(\"producer_id\") == \"2\").withWatermark(\"event_time\", \"90 minutes\")\n",
    "stream_C = parsed_s.filter(col(\"producer_id\") == \"3\").withWatermark(\"event_time\", \"90 minutes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "42b36b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch Speed Limits \n",
    "A_speed = cam_s[1][\"speed_limit\"]\n",
    "B_speed = cam_s[2][\"speed_limit\"]\n",
    "C_speed = cam_s[3][\"speed_limit\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0414b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section renames key fields in each camera stream with numeric suffixes (_1, _2, _3) \n",
    "to indicate their origin (Camera A, B, or C). This naming convention ensures clear traceability \n",
    "and prevents column conflicts during join operations between streams.\n",
    "Each renamed stream is also aliased ('a', 'b', 'c') for easier referencing in joins.\n",
    "\"\"\"\n",
    "\n",
    "# Rename Columns \n",
    "A = stream_A.selectExpr(\n",
    "    \"event_time as event_time_1\",\n",
    "    \"event_id as event_id_1\",\n",
    "    \"batch_id as batch_id_1\",\n",
    "    \"car_plate as car_plate_1\",\n",
    "    \"speed_reading as speed_1\",\n",
    "    \"producer_id as producer_id_1\"\n",
    ").alias(\"a\")\n",
    "\n",
    "B = stream_B.selectExpr(\n",
    "    \"event_time as event_time_2\",\n",
    "    \"event_id as event_id_2\",\n",
    "    \"batch_id as batch_id_2\",\n",
    "    \"car_plate as car_plate_2\",\n",
    "    \"speed_reading as speed_2\",\n",
    "    \"producer_id as producer_id_2\"\n",
    ").alias(\"b\")\n",
    "\n",
    "C = stream_C.selectExpr(\n",
    "    \"event_time as event_time_3\",\n",
    "    \"event_id as event_id_3\",\n",
    "    \"batch_id as batch_id_3\",\n",
    "    \"car_plate as car_plate_3\",\n",
    "    \"speed_reading as speed_3\",\n",
    "    \"producer_id as producer_id_3\"\n",
    ").alias(\"c\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c1baa867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time_1: timestamp (nullable = true)\n",
      " |-- event_id_1: string (nullable = true)\n",
      " |-- batch_id_1: integer (nullable = true)\n",
      " |-- car_plate_1: string (nullable = true)\n",
      " |-- speed_1: double (nullable = true)\n",
      " |-- producer_id_1: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7f583de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section filters each camera stream (A, B and C) to detect single-point speed violations\n",
    "where a vehicle exceeds the predefined speed limit. Each violation stream is then standardized\n",
    "to a common schema using a helper function, making them compatible for union and downstream processing.\n",
    "\n",
    "- `standardize_violation()` extracts and renames fields consistently.\n",
    "- Standardized DataFrames (`A_std`, `B_std`, `C_std`) are then combined using `union()`\n",
    "  to form a single unified stream, `violations_union`, containing all single-camera violations.\n",
    "\"\"\"\n",
    "# Single Camera Speed Violations\n",
    "A_violate = A.filter(col(\"speed_1\") > A_speed)\n",
    "B_violate = B.filter(col(\"speed_2\") > B_speed)\n",
    "C_violate = C.filter(col(\"speed_3\") > C_speed)\n",
    "\n",
    "def standardize_violation(df, suffix):\n",
    "    \"\"\"\n",
    "    Function that standardizes the column names of a violation DataFrame to a common schema\n",
    "    so that it can be merged with violations from other cameras.\n",
    "    \"\"\"\n",
    "    return df.selectExpr(\n",
    "        f\"producer_id_{suffix} as producer_id\",\n",
    "        f\"event_time_{suffix} as event_time\",\n",
    "        f\"event_id_{suffix} as event_id\",\n",
    "        f\"batch_id_{suffix} as batch_id\",\n",
    "        f\"car_plate_{suffix} as car_plate\",\n",
    "        f\"speed_{suffix} as speed_reading\"\n",
    "    )\n",
    "\n",
    "# Apply Standardization\n",
    "A_std = standardize_violation(A_violate, \"1\")\n",
    "B_std = standardize_violation(B_violate, \"2\")\n",
    "C_std = standardize_violation(C_violate, \"3\")\n",
    "\n",
    "# Union of single-point violations\n",
    "violations_union = A_std.union(B_std).union(C_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "23b4b545",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "35f7389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 received at 2025-05-28T07:04:26.655546\n",
      "Empty batch.\n",
      "\n",
      "Batch 1 received at 2025-05-28T07:04:32.307287\n",
      "Rows in batch: 11\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:00:01|d0e547bb-c4a7-4750-b7b4-8076e9b47f4f|1       |CJW 924  |148.3        |\n",
      "|1          |2024-01-01 08:00:02|f3162606-1b2e-407f-951d-61d14c0a7b09|1       |CJP 278  |125.2        |\n",
      "|1          |2024-01-01 08:00:00|5113d990-4da6-41d2-b292-2516e7d8dc07|1       |KKE 15   |150.3        |\n",
      "|1          |2024-01-01 08:00:02|5a4497c9-5e3b-4470-a20c-8ee1f619f732|1       |CZ 592   |154.9        |\n",
      "|1          |2024-01-01 08:00:00|b57e8e44-b6e5-4c1e-bb5b-a4e1bd24bac9|1       |UTT 229  |140.9        |\n",
      "|1          |2024-01-01 08:00:04|c6965fdc-07df-4008-a8d3-f60febcd9223|1       |AH 8     |159.5        |\n",
      "|1          |2024-01-01 08:00:05|f108727e-e97d-4d8e-b109-e7e763b64398|1       |FZ 766   |132.9        |\n",
      "|1          |2024-01-01 08:00:01|7214629f-2fbb-4787-b953-e163720a6c61|1       |KZE 5    |133.3        |\n",
      "|1          |2024-01-01 08:00:03|3ca5e275-0cf6-43a0-8c28-65040c60b8b2|1       |HP 1     |140.0        |\n",
      "|1          |2024-01-01 08:00:04|25a3e2ad-502e-4273-9062-6f00ec63f62c|1       |EOT 00   |142.6        |\n",
      "|1          |2024-01-01 08:00:04|0abc5099-64d1-4b1a-b623-084b19ce9e4e|1       |WA 0712  |142.1        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 2 received at 2025-05-28T07:04:42.339938\n",
      "Rows in batch: 10\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:08:01|be770f2f-e15a-463c-b4a7-955ee3e14924|2       |WB 418   |148.7        |\n",
      "|1          |2024-01-01 08:08:02|d1fcb87d-b48c-43ea-9697-8e6ee3b83cf7|2       |PI 9     |154.0        |\n",
      "|1          |2024-01-01 08:08:01|59e37658-17a9-41e6-8fd1-e65af3e271f9|2       |VWM 13   |115.4        |\n",
      "|1          |2024-01-01 08:08:03|0352cb9f-fc86-4990-abf9-74cfbe287ec8|2       |JEJ 6    |117.6        |\n",
      "|1          |2024-01-01 08:08:01|8017281b-a1a5-4d65-b0e4-7a522876d277|2       |UQV 232  |154.4        |\n",
      "|1          |2024-01-01 08:08:06|8449613c-fd13-4325-a030-4a1891d9aeaa|2       |DSD 320  |113.0        |\n",
      "|1          |2024-01-01 08:08:05|1e7c0e8d-5ebc-4869-9a0b-445c8e6d566e|2       |DWT 789  |129.0        |\n",
      "|1          |2024-01-01 08:08:04|6e662e88-2ece-46d5-b8f6-0c253b0c676a|2       |XYG 05   |114.2        |\n",
      "|1          |2024-01-01 08:08:02|a47e8660-36f5-4cdf-bc7d-b1d709c78974|2       |WZ 24    |158.2        |\n",
      "|1          |2024-01-01 08:08:03|701b86a2-5a29-4bc9-a078-d157fecc486c|2       |PKH 8115 |151.9        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all \n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "a = A_std.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(debug_batch) \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_A\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "try:\n",
    "    a.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping all \")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    a.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ee540333",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./debug_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "21548940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch 0 received at 2025-05-28T07:04:54.864124\n",
      "Empty batch.\n",
      "\n",
      "Batch 1 received at 2025-05-28T07:04:58.154252\n",
      "Rows in batch: 11\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:00:01|d0e547bb-c4a7-4750-b7b4-8076e9b47f4f|1       |CJW 924  |148.3        |\n",
      "|1          |2024-01-01 08:00:02|f3162606-1b2e-407f-951d-61d14c0a7b09|1       |CJP 278  |125.2        |\n",
      "|1          |2024-01-01 08:00:00|5113d990-4da6-41d2-b292-2516e7d8dc07|1       |KKE 15   |150.3        |\n",
      "|1          |2024-01-01 08:00:02|5a4497c9-5e3b-4470-a20c-8ee1f619f732|1       |CZ 592   |154.9        |\n",
      "|1          |2024-01-01 08:00:00|b57e8e44-b6e5-4c1e-bb5b-a4e1bd24bac9|1       |UTT 229  |140.9        |\n",
      "|1          |2024-01-01 08:00:04|c6965fdc-07df-4008-a8d3-f60febcd9223|1       |AH 8     |159.5        |\n",
      "|1          |2024-01-01 08:00:05|f108727e-e97d-4d8e-b109-e7e763b64398|1       |FZ 766   |132.9        |\n",
      "|1          |2024-01-01 08:00:01|7214629f-2fbb-4787-b953-e163720a6c61|1       |KZE 5    |133.3        |\n",
      "|1          |2024-01-01 08:00:03|3ca5e275-0cf6-43a0-8c28-65040c60b8b2|1       |HP 1     |140.0        |\n",
      "|1          |2024-01-01 08:00:04|25a3e2ad-502e-4273-9062-6f00ec63f62c|1       |EOT 00   |142.6        |\n",
      "|1          |2024-01-01 08:00:04|0abc5099-64d1-4b1a-b623-084b19ce9e4e|1       |WA 0712  |142.1        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 2 received at 2025-05-28T07:05:08.288515\n",
      "Rows in batch: 10\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:08:01|be770f2f-e15a-463c-b4a7-955ee3e14924|2       |WB 418   |148.7        |\n",
      "|1          |2024-01-01 08:08:02|d1fcb87d-b48c-43ea-9697-8e6ee3b83cf7|2       |PI 9     |154.0        |\n",
      "|1          |2024-01-01 08:08:01|59e37658-17a9-41e6-8fd1-e65af3e271f9|2       |VWM 13   |115.4        |\n",
      "|1          |2024-01-01 08:08:03|0352cb9f-fc86-4990-abf9-74cfbe287ec8|2       |JEJ 6    |117.6        |\n",
      "|1          |2024-01-01 08:08:01|8017281b-a1a5-4d65-b0e4-7a522876d277|2       |UQV 232  |154.4        |\n",
      "|1          |2024-01-01 08:08:06|8449613c-fd13-4325-a030-4a1891d9aeaa|2       |DSD 320  |113.0        |\n",
      "|1          |2024-01-01 08:08:05|1e7c0e8d-5ebc-4869-9a0b-445c8e6d566e|2       |DWT 789  |129.0        |\n",
      "|1          |2024-01-01 08:08:04|6e662e88-2ece-46d5-b8f6-0c253b0c676a|2       |XYG 05   |114.2        |\n",
      "|1          |2024-01-01 08:08:02|a47e8660-36f5-4cdf-bc7d-b1d709c78974|2       |WZ 24    |158.2        |\n",
      "|1          |2024-01-01 08:08:03|701b86a2-5a29-4bc9-a078-d157fecc486c|2       |PKH 8115 |151.9        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 3 received at 2025-05-28T07:05:18.262575\n",
      "Rows in batch: 12\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:13:08|b05ac078-6f38-41f4-a126-a7616870029d|3       |CIY 810  |132.1        |\n",
      "|1          |2024-01-01 08:13:10|8d513b56-0bf4-467c-add2-77381a36f231|3       |ARX 7573 |121.2        |\n",
      "|1          |2024-01-01 08:13:07|c14cd235-dd29-430d-8419-bd4e5e9c8b13|3       |ZQ 22    |117.1        |\n",
      "|1          |2024-01-01 08:13:11|762873cc-b78d-4813-ba0f-19926f52f06d|3       |VJX 7    |119.6        |\n",
      "|1          |2024-01-01 08:13:07|fb704780-229d-4d81-8afe-e766a61bf515|3       |RK 76    |127.8        |\n",
      "|1          |2024-01-01 08:13:12|2071717d-2db4-469c-b429-bde9741a5c16|3       |QBF 1    |130.7        |\n",
      "|1          |2024-01-01 08:13:11|d7348b88-de55-461c-85bd-1ca9f83814e8|3       |GL 4     |123.0        |\n",
      "|1          |2024-01-01 08:13:11|a0a61300-7bcb-4d68-9b75-856ed74d43e4|3       |MZ 242   |143.3        |\n",
      "|1          |2024-01-01 08:13:12|d306cb22-bc82-4941-8311-9729f26137ab|3       |EC 40    |140.7        |\n",
      "|1          |2024-01-01 08:13:07|f60a87ed-4636-46b8-8d09-35eb73cb559f|3       |SS 1621  |151.8        |\n",
      "|1          |2024-01-01 08:13:12|04cd9a63-9d02-450c-bb88-e0ac7c1a5f9f|3       |XCC 09   |135.6        |\n",
      "|1          |2024-01-01 08:13:07|79a68ea7-182f-414b-ad9e-3ad21c2de074|3       |NGP 66   |113.1        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 4 received at 2025-05-28T07:05:28.283565\n",
      "Rows in batch: 9\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:19:48|9478323b-a2f6-4592-8c9f-a9de5bb984ab|4       |ZEA 3530 |153.3        |\n",
      "|1          |2024-01-01 08:19:48|3e613964-b717-44a8-bf81-a1290ae07a35|4       |SJ 15    |146.2        |\n",
      "|1          |2024-01-01 08:19:49|9f538681-69f0-4efd-a3e3-ef44ee2c1094|4       |PB 55    |111.2        |\n",
      "|1          |2024-01-01 08:19:45|2df3e3fe-0abd-4be4-8386-7cbb6881b520|4       |YXA 7534 |152.6        |\n",
      "|1          |2024-01-01 08:19:46|a342f327-e726-449c-84a2-3fa63f6c1bd4|4       |TZQ 7586 |137.7        |\n",
      "|1          |2024-01-01 08:19:44|e54bef88-8517-4925-966a-cf7b7e0e0b85|4       |CDC 5769 |144.3        |\n",
      "|1          |2024-01-01 08:19:44|2cedaf61-3ecb-4c24-8c7a-0aa7b3d88f7e|4       |DOR 6    |112.2        |\n",
      "|1          |2024-01-01 08:19:45|8c42d2f9-8c1e-4dc3-9ab1-23e9df3dde27|4       |XY 025   |149.0        |\n",
      "|1          |2024-01-01 08:19:47|7e2c7380-bb0a-4237-a7b0-ada854450864|4       |CI 9861  |124.0        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 5 received at 2025-05-28T07:05:34.307790\n",
      "Rows in batch: 5\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|2          |2024-01-01 08:00:26|b5b7dae7-3bf9-4f75-aea8-398d3bde3a41|1       |CJW 924  |156.8        |\n",
      "|2          |2024-01-01 08:00:26|1649c03b-cd6f-47bd-b4f2-c44fcf60524d|1       |CZ 592   |163.9        |\n",
      "|2          |2024-01-01 08:00:26|a9c6a4b7-a6b6-4bd7-9719-88bd895814f4|1       |UTT 229  |134.1        |\n",
      "|2          |2024-01-01 08:00:27|4e13ec51-02db-442c-b7aa-3a2050dd1ad1|1       |WA 0712  |133.8        |\n",
      "|2          |2024-01-01 08:00:28|74248dd9-c9f8-48d2-94fb-dc5610a0f735|1       |AH 8     |158.9        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 6 received at 2025-05-28T07:05:35.887167\n",
      "Rows in batch: 1\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|2          |2024-01-01 08:00:33|c14b050b-9319-4705-9adc-74c99eff0791|2       |FZ 766   |136.0        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 7 received at 2025-05-28T07:05:36.842719\n",
      "Empty batch.\n",
      "\n",
      "Batch 8 received at 2025-05-28T07:05:37.313903\n",
      "Rows in batch: 4\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|2          |2024-01-01 08:00:28|9f2e4a15-24fb-4879-9129-5800338088fa|4       |EOT 00   |138.8        |\n",
      "|2          |2024-01-01 08:00:28|ee787113-eb0d-49d6-9cf9-054569625449|4       |HP 1     |142.8        |\n",
      "|2          |2024-01-01 08:00:29|cada35fc-33a8-4bf0-a640-e0d6fc2a73df|4       |KZE 5    |135.2        |\n",
      "|2          |2024-01-01 08:00:31|ac84bb22-32be-4993-a655-49874adeecc4|4       |CJP 278  |117.6        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 9 received at 2025-05-28T07:05:38.321135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in batch: 14\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|1          |2024-01-01 08:25:21|7399dc47-301e-4fef-83b5-0fb99c533df8|5       |IMU 122  |135.2        |\n",
      "|1          |2024-01-01 08:25:21|654727a6-0002-46dd-ab19-8e468c001848|5       |GPR 4    |149.3        |\n",
      "|1          |2024-01-01 08:25:22|42132abc-2c2e-4624-ab61-0769297f177b|5       |OW 5     |137.3        |\n",
      "|1          |2024-01-01 08:25:21|e408f4bc-a7df-49bf-9523-1f5af6ec4627|5       |REP 98   |129.9        |\n",
      "|1          |2024-01-01 08:25:20|97638b79-22e5-4d9f-9a37-12618005014b|5       |AFS 0    |134.1        |\n",
      "|1          |2024-01-01 08:25:22|f302f720-927b-4c9e-b446-63000d364de8|5       |ZLY 15   |148.6        |\n",
      "|1          |2024-01-01 08:25:21|eb604110-7b16-4729-8aad-17fcc31bc056|5       |DGT 19   |123.7        |\n",
      "|1          |2024-01-01 08:25:23|2d1f1862-8c71-4a67-b4be-a0be8b98716a|5       |JYA 82   |143.9        |\n",
      "|1          |2024-01-01 08:25:24|261e1e49-07ad-49cc-b4a7-a418792be4a4|5       |ZBR 9    |134.2        |\n",
      "|1          |2024-01-01 08:25:21|b8ba6caf-a47b-44dc-8cfd-51d206bdf4c7|5       |QQ 6161  |126.1        |\n",
      "|1          |2024-01-01 08:25:24|d6b659fd-f2a3-41e2-afd3-3ace41aca481|5       |FQ 3     |158.9        |\n",
      "|1          |2024-01-01 08:25:25|255163b8-82ee-4658-b0e5-0607a95f5b28|5       |YE 6517  |126.7        |\n",
      "|1          |2024-01-01 08:25:21|beece8a5-5faf-46e3-91ca-c7e33d931f8e|5       |AW 965   |127.6        |\n",
      "|2          |2024-01-01 08:00:40|2b797755-2cb6-449f-b580-1dd691334076|5       |CX 6145  |110.5        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 10 received at 2025-05-28T07:05:40.434721\n",
      "Rows in batch: 3\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|2          |2024-01-01 08:08:24|7dff112a-d3ce-4ebb-841f-ea3e03662ead|7       |WB 418   |138.8        |\n",
      "|2          |2024-01-01 08:08:26|1f739fcb-8e46-4c92-a69b-9a18533cb646|7       |PI 9     |140.9        |\n",
      "|2          |2024-01-01 08:08:28|756a47f3-7f81-4e17-88fa-b2bbed9aa29e|7       |PKH 8115 |143.8        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 11 received at 2025-05-28T07:05:41.837362\n",
      "Rows in batch: 1\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|producer_id|event_time         |event_id                            |batch_id|car_plate|speed_reading|\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "|2          |2024-01-01 08:08:34|eb9614ad-ee22-4001-835f-4242e362be01|8       |JEJ 6    |111.2        |\n",
      "+-----------+-------------------+------------------------------------+--------+---------+-------------+\n",
      "\n",
      "\n",
      "Batch 12 received at 2025-05-28T07:05:42.907931\n",
      "Empty batch.\n",
      "\n",
      "Batch 13 received at 2025-05-28T07:05:43.356609\n",
      "Empty batch.\n",
      "\n",
      "Batch 14 received at 2025-05-28T07:05:44.360356\n",
      "Empty batch.\n",
      "\n",
      "Batch 15 received at 2025-05-28T07:05:45.399576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_43782/62083479.py\", line 3, in debug_batch\n",
      "    count = df.count()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 804, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1172.count.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3161)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3160)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3160)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all \n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "u = violations_union.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(debug_batch) \\\n",
    "    .option(\"checkpointLocation\", \"./debug_batch\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    u.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping all \")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    u.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c45bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section performs inter-camera joins to detect speed violations that occur \n",
    "between two different cameras—specifically, between Camera A & B (AB) and Camera B & C (BC).\n",
    "\n",
    "## AB Join\n",
    "- Joins records from Camera A and B based on matching `car_plate`.\n",
    "- Ensures Camera B’s event time is after A’s and within 1 hour.\n",
    "- Calculates the average speed between `event_time_1` and `event_time_2`.\n",
    "- Filters out violations where the calculated `avg_speed` exceeds the speed limit \n",
    "  retrieved from MongoDB for Camera B. \n",
    "\n",
    "## BC Join\n",
    "- Applies the same logic between Camera B and C.\n",
    "- Ensures event order and time window between Camera B and C.\n",
    "- Filters out violations where average speed exceeds the limit defined for Camera C. \n",
    "\n",
    "This method captures potential speeding events across multiple camera checkpoints \n",
    "by calculating travel speed over time, making it more reliable than single-point detection.\n",
    "\"\"\"\n",
    "\n",
    "# AB join\n",
    "AB_violation = A.join(\n",
    "    B,\n",
    "    (col(\"a.car_plate_1\") == col(\"b.car_plate_2\")) &\n",
    "    (col(\"b.event_time_2\") > col(\"a.event_time_1\")) &\n",
    "    (col(\"b.event_time_2\") <= col(\"a.event_time_1\") + expr(\"INTERVAL 1 HOUR\")),\n",
    "    how=\"inner\"\n",
    ").selectExpr(\n",
    "    \"a.event_id_1\", \"a.batch_id_1\", \"a.speed_1\", \"a.event_time_1\", \"a.producer_id_1\",\n",
    "    \"b.event_id_2\", \"b.batch_id_2\", \"b.speed_2\", \"b.event_time_2\", \"b.producer_id_2\",\n",
    "    \"a.car_plate_1 as car_plate\"\n",
    ").withColumn(\n",
    "    \"avg_speed\",\n",
    "    1 / ((unix_timestamp(col(\"b.event_time_2\")) - unix_timestamp(col(\"a.event_time_1\"))) / 3600)\n",
    ").filter(\n",
    "    col(\"avg_speed\") > cam_s[2]['speed_limit']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# BC join\n",
    "BC_violation = B.join(\n",
    "    C,\n",
    "    (col(\"b.car_plate_2\") == col(\"c.car_plate_3\")) &\n",
    "    (col(\"c.event_time_3\") > col(\"b.event_time_2\")) &\n",
    "    (col(\"c.event_time_3\") <= col(\"b.event_time_2\") + expr(\"INTERVAL 1 HOUR\")),\n",
    "    how=\"inner\"\n",
    ").selectExpr(\n",
    "    \"b.event_id_2\", \"b.batch_id_2\", \"b.speed_2\", \"b.event_time_2\", \"b.producer_id_2\",\n",
    "    \"c.event_id_3\", \"c.batch_id_3\", \"c.speed_3\", \"c.event_time_3\", \"c.producer_id_3\",\n",
    "    \"b.car_plate_2 as car_plate\"\n",
    ").withColumn(\n",
    "    \"avg_speed\",\n",
    "    1 / ((unix_timestamp(col(\"c.event_time_3\")) - unix_timestamp(col(\"b.event_time_2\"))) / 3600)\n",
    ").filter(\n",
    "    col(\"avg_speed\") > cam_s[3]['speed_limit']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "729dd284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A_event_time: timestamp (nullable = true)\n",
      " |-- A_event_id: string (nullable = true)\n",
      " |-- A_batch_id: integer (nullable = true)\n",
      " |-- A_car_plate: string (nullable = true)\n",
      " |-- A_speed: double (nullable = true)\n",
      " |-- A_producer_id: integer (nullable = true)\n",
      " |-- B_event_time: timestamp (nullable = true)\n",
      " |-- B_event_id: string (nullable = true)\n",
      " |-- B_batch_id: integer (nullable = true)\n",
      " |-- B_car_plate: string (nullable = true)\n",
      " |-- B_speed: double (nullable = true)\n",
      " |-- B_producer_id: integer (nullable = true)\n",
      " |-- avg_speed_h: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AB_violate.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "790f08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_join_batch(df, batch_id):\n",
    "    print(\"\\n\" + \"=\" * 90)\n",
    "    print(f\"Join VIOLATION DEBUG | Batch ID: {batch_id} | Logged at: {datetime.utcnow().isoformat()}\")\n",
    "\n",
    "    record_count = df.count()\n",
    "    if record_count == 0:\n",
    "        print(\"No violations detected in this batch.\")\n",
    "    else:\n",
    "        print(f\"Violations Detected: {record_count}\")\n",
    "        print(\"\\nSample of Joined Violation Records:\\n\")\n",
    "        df.show(truncate=False, n=20)\n",
    "\n",
    "    print(\"=\" * 90)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d4036069",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./ab_join_violate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1de17328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 0 | Logged at: 2025-05-28T07:05:58.203746\n",
      "No violations detected in this batch.\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 1 | Logged at: 2025-05-28T07:06:06.689579\n",
      "No violations detected in this batch.\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 2 | Logged at: 2025-05-28T07:06:13.174224\n",
      "No violations detected in this batch.\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 3 | Logged at: 2025-05-28T07:06:19.931652\n",
      "No violations detected in this batch.\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 4 | Logged at: 2025-05-28T07:06:26.895016\n",
      "No violations detected in this batch.\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 5 | Logged at: 2025-05-28T07:06:33.713204\n",
      "Violations Detected: 1\n",
      "\n",
      "Sample of Joined Violation Records:\n",
      "\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "|event_id_1                          |batch_id_1|speed_1|event_time_1       |producer_id_1|event_id_2                          |batch_id_2|speed_2|event_time_2       |producer_id_2|car_plate|avg_speed         |\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "|2969de1a-a07d-40f5-9d69-0ccf451a94ac|7         |132.5  |2024-01-01 08:42:40|1            |ed683f77-07e1-4d8e-b2f3-805d3ed82091|57        |127.6  |2024-01-01 08:43:06|2            |XWC 903  |138.46153846153848|\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 6 | Logged at: 2025-05-28T07:06:47.541014\n",
      "Violations Detected: 4\n",
      "\n",
      "Sample of Joined Violation Records:\n",
      "\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "|event_id_1                          |batch_id_1|speed_1|event_time_1       |producer_id_1|event_id_2                          |batch_id_2|speed_2|event_time_2       |producer_id_2|car_plate|avg_speed         |\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "|0b93215b-5005-414b-ac61-72f322a1f922|8         |143.4  |2024-01-01 08:51:29|1            |f8c53bc8-ba2a-405a-83fe-3fba52fb4055|70        |138.3  |2024-01-01 08:51:52|2            |EBZ 5064 |156.52173913043478|\n",
      "|1dc969e9-6ef3-48aa-92a9-3dc1868f3bd8|7         |145.1  |2024-01-01 08:42:38|1            |9779f1f5-2105-48e3-8b61-200235c9f60a|66        |137.5  |2024-01-01 08:43:03|2            |ZG 714   |144.0             |\n",
      "|44ba5dff-07cf-45e6-8adc-8c7d35382458|8         |138.7  |2024-01-01 08:51:31|1            |2bba7a1f-e47c-47c7-84be-eb57073b406b|71        |132.6  |2024-01-01 08:51:58|2            |FRG 9    |133.33333333333334|\n",
      "|e7a7614e-a666-489b-a456-1d96593be3ff|8         |157.6  |2024-01-01 08:51:34|1            |01bb358e-a946-43d6-9dbe-7824508d96ff|70        |158.0  |2024-01-01 08:51:55|2            |BCZ 29   |171.42857142857142|\n",
      "+------------------------------------+----------+-------+-------------------+-------------+------------------------------------+----------+-------+-------------------+-------------+---------+------------------+\n",
      "\n",
      "==========================================================================================\n",
      "\n",
      "==========================================================================================\n",
      "Join VIOLATION DEBUG | Batch ID: 7 | Logged at: 2025-05-28T07:07:05.858222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_43782/754746067.py\", line 5, in debug_join_batch\n",
      "    record_count = df.count()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 804, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1244.count.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3161)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3160)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3160)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor65.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all \n"
     ]
    }
   ],
   "source": [
    "ab = AB_violation.writeStream \\\n",
    "    .foreachBatch(debug_join_batch) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./ab_join_violate\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    ab.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping all \")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    ab.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "34461258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_union_batch_to_mongo(df, batch_id, violation_type):\n",
    "    \"\"\"\n",
    "    This function writes a batch of traffic violation records to MongoDB. It supports both:\n",
    "    - Join-based violations (AB, BC) → `violation_type = 1`\n",
    "    - Single-point violations (A, B, C) → `violation_type != 1`\n",
    "\n",
    "    Key features:\n",
    "    - Generates a unique `violation_id` per record using car plate and timestamps\n",
    "\n",
    "    - MongoDB checks if a document with the same `car_plate` and `violation_date` already exists.\n",
    "    - If it does, it does not create a new document.\n",
    "    - Instead, it adds the new `violation_record` to the `violation_records` array field using `$addToSet`.\n",
    "    - `$addToSet` ensures that the same violation is not inserted twice, avoiding duplicates even if the stream retries.\n",
    "\n",
    "    - If the document does not exist:\n",
    "    - MongoDB will create a new document with the fields:\n",
    "      - `car_plate`\n",
    "      - `violation_date`\n",
    "      - `violation_records`: initialized with the new violation entry\n",
    "    - This is handled by setting `upsert=True`, which means \"insert if not found\".\n",
    "\n",
    "    \"\"\"\n",
    "    print(f\"\\nWriting batch {batch_id} (type {violation_type}) to MongoDB\")\n",
    "\n",
    "    if df.isEmpty():\n",
    "        print(\"Empty batch.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        mongo_client = MongoClient(\"10.192.0.245\", 27017)\n",
    "        mongo_client.admin.command(\"ping\")\n",
    "        db = mongo_client[\"traffic_monitoring\"]\n",
    "        violation_coll = db[\"violation\"]\n",
    "    except Exception as e:\n",
    "        print(f\" MongoDB connection failed: {e}\")\n",
    "        return\n",
    "\n",
    "    bulk_ops = []\n",
    "\n",
    "    for row in df.collect():\n",
    "        try:\n",
    "            data = row.asDict()\n",
    "            print(data)\n",
    "            car_plate = data[\"car_plate\"]\n",
    "\n",
    "            if violation_type == 1:\n",
    "                # AB or BC violation (multi-camera)\n",
    "                timestamp_start = data[\"event_time_1\"]\n",
    "                timestamp_end = data[\"event_time_2\"]\n",
    "                speed = data[\"avg_speed\"]\n",
    "                camera_id_start = int(data[\"producer_id_1\"])\n",
    "                camera_id_end = int(data[\"producer_id_2\"])\n",
    "            else:\n",
    "                # Single-point violation (A, B, or C)\n",
    "                timestamp_start = data[\"event_time\"]\n",
    "                timestamp_end = data[\"event_time\"]\n",
    "                speed = data[\"speed_reading\"]\n",
    "                camera_id_start = int(data[\"producer_id\"])\n",
    "                camera_id_end = camera_id_start\n",
    "\n",
    "            violation_date = datetime.combine(timestamp_start.date(), datetime.min.time())\n",
    "            violation_id = f\"{car_plate}--{timestamp_end}--{timestamp_start}\"\n",
    "\n",
    "            violation_record = {\n",
    "                \"violation_id\": violation_id,\n",
    "                \"camera_id_start\": camera_id_start,\n",
    "                \"camera_id_end\": camera_id_end,\n",
    "                \"timestamp_start\": timestamp_start,\n",
    "                \"timestamp_end\": timestamp_end,\n",
    "                \"speed_reading\": speed\n",
    "            }\n",
    "\n",
    "            bulk_ops.append(\n",
    "                UpdateOne(\n",
    "                    {\"car_plate\": car_plate, \"violation_date\": violation_date},\n",
    "                    {\"$addToSet\": {\"violation_records\": violation_record}},\n",
    "                    upsert=True\n",
    "                )\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Row error: {e}\")\n",
    "\n",
    "    try:\n",
    "        if bulk_ops:\n",
    "            violation_coll.bulk_write(bulk_ops, ordered=False)\n",
    "            print(f\"Wrote {len(bulk_ops)} violations to MongoDB.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Bulk write error: {e}\")\n",
    "    finally:\n",
    "        mongo_client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bf0e3f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_union_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe35077c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation count BEFORE stream: 50000\n",
      "\n",
      "Writing batch 0 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 1 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 2 (type 2) to MongoDB\n",
      "Wrote 2 violations to MongoDB.\n",
      "\n",
      "Writing batch 3 (type 2) to MongoDB\n",
      "Wrote 2 violations to MongoDB.\n",
      "\n",
      "Writing batch 4 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 5 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 6 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 7 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 8 (type 2) to MongoDB\n",
      "Wrote 7 violations to MongoDB.\n",
      "\n",
      "Writing batch 9 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 10 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 11 (type 2) to MongoDB\n",
      "Wrote 1 violations to MongoDB.\n",
      "\n",
      "Writing batch 12 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 13 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 14 (type 2) to MongoDB\n",
      "Wrote 5 violations to MongoDB.\n",
      "\n",
      "Writing batch 15 (type 2) to MongoDB\n",
      "Wrote 2 violations to MongoDB.\n",
      "\n",
      "Writing batch 16 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 17 (type 2) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 18 (type 2) to MongoDB\n",
      "Wrote 10 violations to MongoDB.\n",
      "\n",
      "Writing batch 19 (type 2) to MongoDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\n",
      "    raise e\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\n",
      "    self.func(DataFrame(jdf, self.session), batch_id)\n",
      "  File \"/tmp/ipykernel_43782/2216886673.py\", line 6, in <lambda>\n",
      "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 2)) \\\n",
      "  File \"/tmp/ipykernel_43782/3386662813.py\", line 4, in write_union_batch_to_mongo\n",
      "    if df.isEmpty():\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/dataframe.py\", line 553, in isEmpty\n",
      "    return self._jdf.isEmpty()\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o975.isEmpty.\n",
      ": java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1(Dataset.scala:605)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$isEmpty$1$adapted(Dataset.scala:604)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\n",
      "\tat org.apache.spark.sql.Dataset.isEmpty(Dataset.scala:604)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor103.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
      "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
      "\tat jdk.proxy3/jdk.proxy3.$Proxy36.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:51)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:32)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:660)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:658)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:255)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:218)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:212)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interrupted by CTRL-C. Stopping all queries.\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "violation_coll = db[\"violation\"]\n",
    "before_count = violation_coll.count_documents({})\n",
    "print(f\"Violation count BEFORE stream: {before_count}\")\n",
    "\n",
    "single = single = violations_union.writeStream \\\n",
    "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 2)) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_union_writer\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    single.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping all \")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    single.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e5f5101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation count AFTER stream: 50034\n",
      "New records added: 34\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "after_count = violation_coll.count_documents({})\n",
    "print(f\"Violation count AFTER stream: {after_count}\")\n",
    "print(f\"New records added: {after_count - before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5ea43f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_join_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113804c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation count BEFORE stream: 50034\n",
      "\n",
      "Writing batch 0 (type 1) to MongoDB\n",
      "Empty batch.\n",
      "\n",
      "Writing batch 1 (type 1) to MongoDB\n",
      "{'event_id_1': 'f3162606-1b2e-407f-951d-61d14c0a7b09', 'batch_id_1': 1, 'speed_1': 125.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 2), 'producer_id_1': 1, 'event_id_2': 'ac84bb22-32be-4993-a655-49874adeecc4', 'batch_id_2': 4, 'speed_2': 117.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 31), 'producer_id_2': 2, 'car_plate': 'CJP 278', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': '3ca5e275-0cf6-43a0-8c28-65040c60b8b2', 'batch_id_1': 1, 'speed_1': 140.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 3), 'producer_id_1': 1, 'event_id_2': 'ee787113-eb0d-49d6-9cf9-054569625449', 'batch_id_2': 4, 'speed_2': 142.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 28), 'producer_id_2': 2, 'car_plate': 'HP 1', 'avg_speed': 144.0}\n",
      "{'event_id_1': '7214629f-2fbb-4787-b953-e163720a6c61', 'batch_id_1': 1, 'speed_1': 133.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 1), 'producer_id_1': 1, 'event_id_2': 'cada35fc-33a8-4bf0-a640-e0d6fc2a73df', 'batch_id_2': 4, 'speed_2': 135.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 29), 'producer_id_2': 2, 'car_plate': 'KZE 5', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': 'b57e8e44-b6e5-4c1e-bb5b-a4e1bd24bac9', 'batch_id_1': 1, 'speed_1': 140.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0), 'producer_id_1': 1, 'event_id_2': 'a9c6a4b7-a6b6-4bd7-9719-88bd895814f4', 'batch_id_2': 1, 'speed_2': 134.1, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 26), 'producer_id_2': 2, 'car_plate': 'UTT 229', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': 'd0e547bb-c4a7-4750-b7b4-8076e9b47f4f', 'batch_id_1': 1, 'speed_1': 148.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 1), 'producer_id_1': 1, 'event_id_2': 'b5b7dae7-3bf9-4f75-aea8-398d3bde3a41', 'batch_id_2': 1, 'speed_2': 156.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 26), 'producer_id_2': 2, 'car_plate': 'CJW 924', 'avg_speed': 144.0}\n",
      "{'event_id_1': 'c6965fdc-07df-4008-a8d3-f60febcd9223', 'batch_id_1': 1, 'speed_1': 159.5, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 4), 'producer_id_1': 1, 'event_id_2': '74248dd9-c9f8-48d2-94fb-dc5610a0f735', 'batch_id_2': 1, 'speed_2': 158.9, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 28), 'producer_id_2': 2, 'car_plate': 'AH 8', 'avg_speed': 150.0}\n",
      "{'event_id_1': '25a3e2ad-502e-4273-9062-6f00ec63f62c', 'batch_id_1': 1, 'speed_1': 142.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 4), 'producer_id_1': 1, 'event_id_2': '9f2e4a15-24fb-4879-9129-5800338088fa', 'batch_id_2': 4, 'speed_2': 138.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 28), 'producer_id_2': 2, 'car_plate': 'EOT 00', 'avg_speed': 150.0}\n",
      "{'event_id_1': '0abc5099-64d1-4b1a-b623-084b19ce9e4e', 'batch_id_1': 1, 'speed_1': 142.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 4), 'producer_id_1': 1, 'event_id_2': '4e13ec51-02db-442c-b7aa-3a2050dd1ad1', 'batch_id_2': 1, 'speed_2': 133.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 27), 'producer_id_2': 2, 'car_plate': 'WA 0712', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': 'f108727e-e97d-4d8e-b109-e7e763b64398', 'batch_id_1': 1, 'speed_1': 132.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 5), 'producer_id_1': 1, 'event_id_2': 'c14b050b-9319-4705-9adc-74c99eff0791', 'batch_id_2': 2, 'speed_2': 136.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 33), 'producer_id_2': 2, 'car_plate': 'FZ 766', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': '5a4497c9-5e3b-4470-a20c-8ee1f619f732', 'batch_id_1': 1, 'speed_1': 154.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0, 2), 'producer_id_1': 1, 'event_id_2': '1649c03b-cd6f-47bd-b4f2-c44fcf60524d', 'batch_id_2': 1, 'speed_2': 163.9, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 26), 'producer_id_2': 2, 'car_plate': 'CZ 592', 'avg_speed': 150.0}\n",
      "Wrote 10 violations to MongoDB.\n",
      "\n",
      "Writing batch 2 (type 1) to MongoDB\n",
      "{'event_id_1': 'd1fcb87d-b48c-43ea-9697-8e6ee3b83cf7', 'batch_id_1': 2, 'speed_1': 154.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 2), 'producer_id_1': 1, 'event_id_2': '1f739fcb-8e46-4c92-a69b-9a18533cb646', 'batch_id_2': 7, 'speed_2': 140.9, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 26), 'producer_id_2': 2, 'car_plate': 'PI 9', 'avg_speed': 150.0}\n",
      "{'event_id_1': '59e37658-17a9-41e6-8fd1-e65af3e271f9', 'batch_id_1': 2, 'speed_1': 115.4, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 1), 'producer_id_1': 1, 'event_id_2': '340aef07-48be-44d0-aeb8-b08df2023f7e', 'batch_id_2': 16, 'speed_2': 117.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 31), 'producer_id_2': 2, 'car_plate': 'VWM 13', 'avg_speed': 120.0}\n",
      "{'event_id_1': '0352cb9f-fc86-4990-abf9-74cfbe287ec8', 'batch_id_1': 2, 'speed_1': 117.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 3), 'producer_id_1': 1, 'event_id_2': 'eb9614ad-ee22-4001-835f-4242e362be01', 'batch_id_2': 8, 'speed_2': 111.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 34), 'producer_id_2': 2, 'car_plate': 'JEJ 6', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': 'be770f2f-e15a-463c-b4a7-955ee3e14924', 'batch_id_1': 2, 'speed_1': 148.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 1), 'producer_id_1': 1, 'event_id_2': '7dff112a-d3ce-4ebb-841f-ea3e03662ead', 'batch_id_2': 7, 'speed_2': 138.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 24), 'producer_id_2': 2, 'car_plate': 'WB 418', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '5113d990-4da6-41d2-b292-2516e7d8dc07', 'batch_id_1': 1, 'speed_1': 150.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 0), 'producer_id_1': 1, 'event_id_2': '42b0938d-6524-40b5-99cc-b71e26aa30f3', 'batch_id_2': 12, 'speed_2': 138.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 0, 23), 'producer_id_2': 2, 'car_plate': 'KKE 15', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '701b86a2-5a29-4bc9-a078-d157fecc486c', 'batch_id_1': 2, 'speed_1': 151.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 3), 'producer_id_1': 1, 'event_id_2': '756a47f3-7f81-4e17-88fa-b2bbed9aa29e', 'batch_id_2': 7, 'speed_2': 143.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 28), 'producer_id_2': 2, 'car_plate': 'PKH 8115', 'avg_speed': 144.0}\n",
      "{'event_id_1': '6e662e88-2ece-46d5-b8f6-0c253b0c676a', 'batch_id_1': 2, 'speed_1': 114.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 4), 'producer_id_1': 1, 'event_id_2': 'd3051697-4d90-4d15-8802-b34bd9436732', 'batch_id_2': 16, 'speed_2': 109.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 33), 'producer_id_2': 2, 'car_plate': 'XYG 05', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': 'a47e8660-36f5-4cdf-bc7d-b1d709c78974', 'batch_id_1': 2, 'speed_1': 158.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 2), 'producer_id_1': 1, 'event_id_2': 'cbafd132-a61f-4713-b2f9-76f08141c70e', 'batch_id_2': 15, 'speed_2': 160.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 25), 'producer_id_2': 2, 'car_plate': 'WZ 24', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '8017281b-a1a5-4d65-b0e4-7a522876d277', 'batch_id_1': 2, 'speed_1': 154.4, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 1), 'producer_id_1': 1, 'event_id_2': '9dd51204-9367-4369-8575-611b817ce296', 'batch_id_2': 15, 'speed_2': 157.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 25), 'producer_id_2': 2, 'car_plate': 'UQV 232', 'avg_speed': 150.0}\n",
      "Wrote 9 violations to MongoDB.\n",
      "\n",
      "Writing batch 3 (type 1) to MongoDB\n",
      "{'event_id_1': 'a0a61300-7bcb-4d68-9b75-856ed74d43e4', 'batch_id_1': 3, 'speed_1': 143.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 11), 'producer_id_1': 1, 'event_id_2': 'c6a47514-5b97-4347-adfd-ba7a756615d5', 'batch_id_2': 25, 'speed_2': 152.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 34), 'producer_id_2': 2, 'car_plate': 'MZ 242', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '79a68ea7-182f-414b-ad9e-3ad21c2de074', 'batch_id_1': 3, 'speed_1': 113.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 7), 'producer_id_1': 1, 'event_id_2': '1622ac4e-d0d1-4713-85fe-e5b5f01d1c28', 'batch_id_2': 19, 'speed_2': 107.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 38), 'producer_id_2': 2, 'car_plate': 'NGP 66', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': '013b5963-adc3-426b-b6f4-2c649aef7630', 'batch_id_1': 2, 'speed_1': 104.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 2), 'producer_id_1': 1, 'event_id_2': 'a5cea7f7-ffa2-497d-9ae2-305343afa884', 'batch_id_2': 22, 'speed_2': 101.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 34), 'producer_id_2': 2, 'car_plate': 'VM 4837', 'avg_speed': 112.5}\n",
      "{'event_id_1': '762873cc-b78d-4813-ba0f-19926f52f06d', 'batch_id_1': 3, 'speed_1': 119.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 11), 'producer_id_1': 1, 'event_id_2': 'ab8a8200-a871-4c97-8728-e3a4affdec46', 'batch_id_2': 26, 'speed_2': 116.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 42), 'producer_id_2': 2, 'car_plate': 'VJX 7', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': '8449613c-fd13-4325-a030-4a1891d9aeaa', 'batch_id_1': 2, 'speed_1': 113.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 6), 'producer_id_1': 1, 'event_id_2': '6eff696a-810d-40ab-99b0-860b8eb43e97', 'batch_id_2': 22, 'speed_2': 117.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 36), 'producer_id_2': 2, 'car_plate': 'DSD 320', 'avg_speed': 120.0}\n",
      "{'event_id_1': 'c14cd235-dd29-430d-8419-bd4e5e9c8b13', 'batch_id_1': 3, 'speed_1': 117.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 7), 'producer_id_1': 1, 'event_id_2': 'd40dc066-ba11-4d44-a4e5-48f7038544a7', 'batch_id_2': 19, 'speed_2': 113.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 36), 'producer_id_2': 2, 'car_plate': 'ZQ 22', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': '1e7c0e8d-5ebc-4869-9a0b-445c8e6d566e', 'batch_id_1': 2, 'speed_1': 129.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 8, 5), 'producer_id_1': 1, 'event_id_2': '3d5110f0-2eb0-4ff6-8319-53677a95fb5c', 'batch_id_2': 22, 'speed_2': 135.4, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 8, 34), 'producer_id_2': 2, 'car_plate': 'DWT 789', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': 'd306cb22-bc82-4941-8311-9729f26137ab', 'batch_id_1': 3, 'speed_1': 140.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 12), 'producer_id_1': 1, 'event_id_2': '40d72911-cdde-4af8-9a18-68aa908a46a5', 'batch_id_2': 19, 'speed_2': 144.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 37), 'producer_id_2': 2, 'car_plate': 'EC 40', 'avg_speed': 144.0}\n",
      "{'event_id_1': 'b05ac078-6f38-41f4-a126-a7616870029d', 'batch_id_1': 3, 'speed_1': 132.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 8), 'producer_id_1': 1, 'event_id_2': '315c64ac-0f96-4a67-98c4-7d7b85cfa455', 'batch_id_2': 19, 'speed_2': 136.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 34), 'producer_id_2': 2, 'car_plate': 'CIY 810', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': '2071717d-2db4-469c-b429-bde9741a5c16', 'batch_id_1': 3, 'speed_1': 130.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 12), 'producer_id_1': 1, 'event_id_2': '97de7ab2-ba78-4c85-9606-2529a89c37d2', 'batch_id_2': 26, 'speed_2': 123.4, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 40), 'producer_id_2': 2, 'car_plate': 'QBF 1', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': '04cd9a63-9d02-450c-bb88-e0ac7c1a5f9f', 'batch_id_1': 3, 'speed_1': 135.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 12), 'producer_id_1': 1, 'event_id_2': '9f2babdd-5d6d-4d44-a682-0bd264135fb2', 'batch_id_2': 25, 'speed_2': 138.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 37), 'producer_id_2': 2, 'car_plate': 'XCC 09', 'avg_speed': 144.0}\n",
      "Wrote 11 violations to MongoDB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing batch 4 (type 1) to MongoDB\n",
      "{'event_id_1': '7e2c7380-bb0a-4237-a7b0-ada854450864', 'batch_id_1': 4, 'speed_1': 124.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 47), 'producer_id_1': 1, 'event_id_2': '7a0ed97c-8aeb-413b-831e-bf10dc172ada', 'batch_id_2': 37, 'speed_2': 118.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 15), 'producer_id_2': 2, 'car_plate': 'CI 9861', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': '2cedaf61-3ecb-4c24-8c7a-0aa7b3d88f7e', 'batch_id_1': 4, 'speed_1': 112.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 44), 'producer_id_1': 1, 'event_id_2': '1766270c-3293-4b56-9fef-8b16250fe1c3', 'batch_id_2': 37, 'speed_2': 106.1, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 15), 'producer_id_2': 2, 'car_plate': 'DOR 6', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': 'fb704780-229d-4d81-8afe-e766a61bf515', 'batch_id_1': 3, 'speed_1': 127.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 7), 'producer_id_1': 1, 'event_id_2': '249a31d7-c3d8-4734-977c-73cacf0b76ab', 'batch_id_2': 34, 'speed_2': 135.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 35), 'producer_id_2': 2, 'car_plate': 'RK 76', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': '9478323b-a2f6-4592-8c9f-a9de5bb984ab', 'batch_id_1': 4, 'speed_1': 153.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 48), 'producer_id_1': 1, 'event_id_2': '007926df-1bc5-40b0-b851-409d11c56013', 'batch_id_2': 37, 'speed_2': 148.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 11), 'producer_id_2': 2, 'car_plate': 'ZEA 3530', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '8c42d2f9-8c1e-4dc3-9ab1-23e9df3dde27', 'batch_id_1': 4, 'speed_1': 149.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 45), 'producer_id_1': 1, 'event_id_2': '62c94001-08b5-4ccf-904a-b75306211865', 'batch_id_2': 30, 'speed_2': 144.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 9), 'producer_id_2': 2, 'car_plate': 'XY 025', 'avg_speed': 150.0}\n",
      "{'event_id_1': '8d513b56-0bf4-467c-add2-77381a36f231', 'batch_id_1': 3, 'speed_1': 121.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 10), 'producer_id_1': 1, 'event_id_2': 'e0a30a32-2019-48f1-8cf1-ec91419b6b44', 'batch_id_2': 35, 'speed_2': 120.4, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 39), 'producer_id_2': 2, 'car_plate': 'ARX 7573', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': 'f60a87ed-4636-46b8-8d09-35eb73cb559f', 'batch_id_1': 3, 'speed_1': 151.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 7), 'producer_id_1': 1, 'event_id_2': '39c0ebcf-64ce-433c-a01b-02feb51ef9b2', 'batch_id_2': 34, 'speed_2': 139.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 32), 'producer_id_2': 2, 'car_plate': 'SS 1621', 'avg_speed': 144.0}\n",
      "{'event_id_1': 'd7348b88-de55-461c-85bd-1ca9f83814e8', 'batch_id_1': 3, 'speed_1': 123.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 13, 11), 'producer_id_1': 1, 'event_id_2': 'c6d66aa7-cdf3-4715-acbe-4874548d30ad', 'batch_id_2': 35, 'speed_2': 116.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 13, 39), 'producer_id_2': 2, 'car_plate': 'GL 4', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': '3e613964-b717-44a8-bf81-a1290ae07a35', 'batch_id_1': 4, 'speed_1': 146.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 48), 'producer_id_1': 1, 'event_id_2': 'e01aeb58-806f-4635-931b-1429a618a34d', 'batch_id_2': 37, 'speed_2': 135.5, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 10), 'producer_id_2': 2, 'car_plate': 'SJ 15', 'avg_speed': 163.63636363636363}\n",
      "{'event_id_1': '2df3e3fe-0abd-4be4-8386-7cbb6881b520', 'batch_id_1': 4, 'speed_1': 152.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 45), 'producer_id_1': 1, 'event_id_2': 'ab220781-1fa1-446d-b53f-b6d55309adbf', 'batch_id_2': 30, 'speed_2': 156.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 9), 'producer_id_2': 2, 'car_plate': 'YXA 7534', 'avg_speed': 150.0}\n",
      "Wrote 10 violations to MongoDB.\n",
      "\n",
      "Writing batch 5 (type 1) to MongoDB\n",
      "{'event_id_1': '8044f404-e5e5-496d-bd76-7e92fb9ad4ff', 'batch_id_1': 6, 'speed_1': 127.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 25), 'producer_id_1': 1, 'event_id_2': '129cb37d-a63b-4fd4-80d0-219ca72453ef', 'batch_id_2': 50, 'speed_2': 135.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 53), 'producer_id_2': 2, 'car_plate': 'IF 7805', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': 'e54bef88-8517-4925-966a-cf7b7e0e0b85', 'batch_id_1': 4, 'speed_1': 144.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 44), 'producer_id_1': 1, 'event_id_2': '94ef728f-729e-4203-a193-114c2bc5a4ba', 'batch_id_2': 45, 'speed_2': 142.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 7), 'producer_id_2': 2, 'car_plate': 'CDC 5769', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': 'f302f720-927b-4c9e-b446-63000d364de8', 'batch_id_1': 5, 'speed_1': 148.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 22), 'producer_id_1': 1, 'event_id_2': '9fcd88c7-0936-4d2f-a1fc-b84626615008', 'batch_id_2': 42, 'speed_2': 150.5, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 46), 'producer_id_2': 2, 'car_plate': 'ZLY 15', 'avg_speed': 150.0}\n",
      "{'event_id_1': '9f538681-69f0-4efd-a3e3-ef44ee2c1094', 'batch_id_1': 4, 'speed_1': 111.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 49), 'producer_id_1': 1, 'event_id_2': '79697b94-8b50-405c-b206-83eb8753d03e', 'batch_id_2': 46, 'speed_2': 106.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 20), 'producer_id_2': 2, 'car_plate': 'PB 55', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': '2f1121c5-318b-41a0-a65d-9063dfa92ba2', 'batch_id_1': 6, 'speed_1': 141.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 23), 'producer_id_1': 1, 'event_id_2': '4747d3e1-3f6a-4c76-8705-747276c52c35', 'batch_id_2': 50, 'speed_2': 139.1, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 49), 'producer_id_2': 2, 'car_plate': 'SAL 1597', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': 'd6b659fd-f2a3-41e2-afd3-3ace41aca481', 'batch_id_1': 5, 'speed_1': 158.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 24), 'producer_id_1': 1, 'event_id_2': '14a74806-d1ce-4c84-a2f1-ea2132bf2e04', 'batch_id_2': 48, 'speed_2': 171.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 48), 'producer_id_2': 2, 'car_plate': 'FQ 3', 'avg_speed': 150.0}\n",
      "{'event_id_1': 'ce2eb21d-580f-48f6-85cb-2f83a8f1ea15', 'batch_id_1': 5, 'speed_1': 109.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 24), 'producer_id_1': 1, 'event_id_2': '988064e2-5e43-434b-868a-c201ca32240a', 'batch_id_2': 43, 'speed_2': 110.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 55), 'producer_id_2': 2, 'car_plate': 'TAD 5898', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': 'a342f327-e726-449c-84a2-3fa63f6c1bd4', 'batch_id_1': 4, 'speed_1': 137.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 19, 46), 'producer_id_1': 1, 'event_id_2': 'f1cf9949-cdef-445d-97f1-ea11d7fca232', 'batch_id_2': 45, 'speed_2': 134.4, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 20, 11), 'producer_id_2': 2, 'car_plate': 'TZQ 7586', 'avg_speed': 144.0}\n",
      "{'event_id_1': '2d1f1862-8c71-4a67-b4be-a0be8b98716a', 'batch_id_1': 5, 'speed_1': 143.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 23), 'producer_id_1': 1, 'event_id_2': '2378fdc8-efb2-4053-873e-9216523f8f06', 'batch_id_2': 42, 'speed_2': 145.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 47), 'producer_id_2': 2, 'car_plate': 'JYA 82', 'avg_speed': 150.0}\n",
      "{'event_id_1': 'e2dd36ff-5c13-4af8-a73d-0547f6224a85', 'batch_id_1': 6, 'speed_1': 125.0, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 23), 'producer_id_1': 1, 'event_id_2': 'd5bae299-29ae-464f-89c5-c7492be45668', 'batch_id_2': 50, 'speed_2': 127.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 50), 'producer_id_2': 2, 'car_plate': 'ZVK 10', 'avg_speed': 133.33333333333334}\n",
      "{'event_id_1': '255163b8-82ee-4658-b0e5-0607a95f5b28', 'batch_id_1': 5, 'speed_1': 126.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 25), 'producer_id_1': 1, 'event_id_2': '83e6886c-ac32-4e31-a7bd-72e7122efb3a', 'batch_id_2': 48, 'speed_2': 121.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 52), 'producer_id_2': 2, 'car_plate': 'YE 6517', 'avg_speed': 133.33333333333334}\n",
      "{'event_id_1': '97638b79-22e5-4d9f-9a37-12618005014b', 'batch_id_1': 5, 'speed_1': 134.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 20), 'producer_id_1': 1, 'event_id_2': '25f89334-492a-43a8-9149-7e48566dff38', 'batch_id_2': 42, 'speed_2': 126.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 46), 'producer_id_2': 2, 'car_plate': 'AFS 0', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': 'b7c015cf-dd89-4bad-89b3-e86440ffadcc', 'batch_id_1': 6, 'speed_1': 115.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': '75a5c4c1-eb0b-4f66-a61a-8edd3383743f', 'batch_id_2': 50, 'speed_2': 113.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 50), 'producer_id_2': 2, 'car_plate': 'WX 31', 'avg_speed': 120.0}\n",
      "{'event_id_1': 'beece8a5-5faf-46e3-91ca-c7e33d931f8e', 'batch_id_1': 5, 'speed_1': 127.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': 'd344d812-84d6-4c47-a920-50f7381028f9', 'batch_id_2': 48, 'speed_2': 120.4, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 49), 'producer_id_2': 2, 'car_plate': 'AW 965', 'avg_speed': 128.57142857142858}\n",
      "{'event_id_1': 'eb604110-7b16-4729-8aad-17fcc31bc056', 'batch_id_1': 5, 'speed_1': 123.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': '4083acd5-d65b-4ef7-a122-8f277d57635d', 'batch_id_2': 42, 'speed_2': 126.5, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 49), 'producer_id_2': 2, 'car_plate': 'DGT 19', 'avg_speed': 128.57142857142858}\n",
      "Wrote 15 violations to MongoDB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing batch 6 (type 1) to MongoDB\n",
      "{'event_id_1': 'd62f5e13-1055-40e7-86a8-ed8c697c4b84', 'batch_id_1': 6, 'speed_1': 149.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 23), 'producer_id_1': 1, 'event_id_2': '574ef9f7-5ca3-4e2d-a323-3f4530e0ce1d', 'batch_id_2': 55, 'speed_2': 153.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 46), 'producer_id_2': 2, 'car_plate': 'TB 5190', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '261e1e49-07ad-49cc-b4a7-a418792be4a4', 'batch_id_1': 5, 'speed_1': 134.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 24), 'producer_id_1': 1, 'event_id_2': 'd01bcc6c-1c6e-4132-ab8c-aef2e2b91136', 'batch_id_2': 52, 'speed_2': 129.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 51), 'producer_id_2': 2, 'car_plate': 'ZBR 9', 'avg_speed': 133.33333333333334}\n",
      "{'event_id_1': '654727a6-0002-46dd-ab19-8e468c001848', 'batch_id_1': 5, 'speed_1': 149.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': '1a665370-9eab-4066-8801-c41ed75e342f', 'batch_id_2': 52, 'speed_2': 149.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 46), 'producer_id_2': 2, 'car_plate': 'GPR 4', 'avg_speed': 144.0}\n",
      "{'event_id_1': 'e408f4bc-a7df-49bf-9523-1f5af6ec4627', 'batch_id_1': 5, 'speed_1': 129.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': '6d8286aa-6178-4f00-978a-98238c94f452', 'batch_id_2': 52, 'speed_2': 132.1, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 47), 'producer_id_2': 2, 'car_plate': 'REP 98', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': '42132abc-2c2e-4624-ab61-0769297f177b', 'batch_id_1': 5, 'speed_1': 137.3, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 22), 'producer_id_1': 1, 'event_id_2': 'ea0d6207-e626-4ce7-bf1f-fb72678c7911', 'batch_id_2': 52, 'speed_2': 132.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 48), 'producer_id_2': 2, 'car_plate': 'OW 5', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': '7399dc47-301e-4fef-83b5-0fb99c533df8', 'batch_id_1': 5, 'speed_1': 135.2, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': 'c19ae57d-181a-4b68-8c42-b1a26bc67480', 'batch_id_2': 52, 'speed_2': 133.1, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 48), 'producer_id_2': 2, 'car_plate': 'IMU 122', 'avg_speed': 133.33333333333334}\n",
      "{'event_id_1': 'b8ba6caf-a47b-44dc-8cfd-51d206bdf4c7', 'batch_id_1': 5, 'speed_1': 126.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 25, 21), 'producer_id_1': 1, 'event_id_2': '077287f1-0ebb-40a9-8e76-69951bcb9980', 'batch_id_2': 52, 'speed_2': 123.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 25, 47), 'producer_id_2': 2, 'car_plate': 'QQ 6161', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': '7d1f6eb8-1c83-443e-8fb7-c86e04e8f4f1', 'batch_id_1': 6, 'speed_1': 120.5, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': 'b88daf50-dfd8-495d-ba04-0625ddccbc85', 'batch_id_2': 55, 'speed_2': 117.8, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 49), 'producer_id_2': 2, 'car_plate': 'MFH 37', 'avg_speed': 124.13793103448276}\n",
      "{'event_id_1': '2969de1a-a07d-40f5-9d69-0ccf451a94ac', 'batch_id_1': 7, 'speed_1': 132.5, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 42, 40), 'producer_id_1': 1, 'event_id_2': 'ed683f77-07e1-4d8e-b2f3-805d3ed82091', 'batch_id_2': 57, 'speed_2': 127.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 43, 6), 'producer_id_2': 2, 'car_plate': 'XWC 903', 'avg_speed': 138.46153846153848}\n",
      "{'event_id_1': '78cb27b1-6cc6-4899-ae74-7297e8811ebf', 'batch_id_1': 6, 'speed_1': 159.6, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 25), 'producer_id_1': 1, 'event_id_2': '9e6d33c3-f430-4585-bcf4-8844641b85d8', 'batch_id_2': 55, 'speed_2': 147.7, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 48), 'producer_id_2': 2, 'car_plate': 'KNT 8153', 'avg_speed': 156.52173913043478}\n",
      "{'event_id_1': '4fb23ec1-4956-466b-9ad9-07d534e0872b', 'batch_id_1': 6, 'speed_1': 132.9, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 22), 'producer_id_1': 1, 'event_id_2': '19ab3e54-d3d0-4f39-8297-642e3c35a624', 'batch_id_2': 55, 'speed_2': 128.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 49), 'producer_id_2': 2, 'car_plate': 'WK 223', 'avg_speed': 133.33333333333334}\n",
      "{'event_id_1': '0b08e2f7-9b0d-4ce6-b846-105e39b80d77', 'batch_id_1': 6, 'speed_1': 133.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': '709719ca-a6f5-4739-af0a-a53e698b68f9', 'batch_id_2': 55, 'speed_2': 124.0, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 45), 'producer_id_2': 2, 'car_plate': 'KNZ 1', 'avg_speed': 144.0}\n",
      "Wrote 12 violations to MongoDB.\n",
      "\n",
      "Writing batch 7 (type 1) to MongoDB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event_id_1': '8ed66d4a-c486-4f81-aba2-747ba57894e2', 'batch_id_1': 6, 'speed_1': 119.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 24), 'producer_id_1': 1, 'event_id_2': 'def44cb3-9553-4637-a9dc-5c33c6c2eab6', 'batch_id_2': 63, 'speed_2': 123.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 55), 'producer_id_2': 2, 'car_plate': 'GNU 052', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': 'dcc2f0f5-7de5-4563-b166-6967c54a0ad5', 'batch_id_1': 6, 'speed_1': 117.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': '9d03b365-b844-4566-80f0-37b662cf8d65', 'batch_id_2': 62, 'speed_2': 117.2, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 51), 'producer_id_2': 2, 'car_plate': 'HMU 040', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': '508db83a-e1cb-4d4b-9c7b-156573739994', 'batch_id_1': 6, 'speed_1': 114.7, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': '3c86113d-ee96-4f1a-bdd6-c296f4d368a9', 'batch_id_2': 62, 'speed_2': 109.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 51), 'producer_id_2': 2, 'car_plate': 'VH 827', 'avg_speed': 116.12903225806451}\n",
      "{'event_id_1': 'b668da0a-8cd1-4baa-90c8-078d2e979cfb', 'batch_id_1': 6, 'speed_1': 112.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 23), 'producer_id_1': 1, 'event_id_2': '6b532fa0-42d3-4a29-9d65-0479e35e4625', 'batch_id_2': 62, 'speed_2': 116.3, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 53), 'producer_id_2': 2, 'car_plate': 'ZCO 026', 'avg_speed': 120.0}\n",
      "{'event_id_1': '1dc969e9-6ef3-48aa-92a9-3dc1868f3bd8', 'batch_id_1': 7, 'speed_1': 145.1, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 42, 38), 'producer_id_1': 1, 'event_id_2': '9779f1f5-2105-48e3-8b61-200235c9f60a', 'batch_id_2': 66, 'speed_2': 137.5, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 43, 3), 'producer_id_2': 2, 'car_plate': 'ZG 714', 'avg_speed': 144.0}\n",
      "{'event_id_1': '04e8dbb7-a0ab-4f03-9dea-97ecc31fc3fc', 'batch_id_1': 6, 'speed_1': 121.8, 'event_time_1': datetime.datetime(2024, 1, 1, 8, 35, 20), 'producer_id_1': 1, 'event_id_2': 'ad09aaf0-d2cf-4280-8352-26c3778cb121', 'batch_id_2': 62, 'speed_2': 128.6, 'event_time_2': datetime.datetime(2024, 1, 1, 8, 35, 49), 'producer_id_2': 2, 'car_plate': 'NJN 8189', 'avg_speed': 124.13793103448276}\n",
      "Wrote 6 violations to MongoDB.\n",
      "Interrupted by CTRL-C. Stopping all queries.\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "violation_coll = db[\"violation\"]\n",
    "before_count = violation_coll.count_documents({})\n",
    "print(f\"Violation count BEFORE stream: {before_count}\")\n",
    "\n",
    "AB_join = AB_violation.writeStream \\\n",
    "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 1)) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_join_writer\") \\\n",
    "    .start()\n",
    "\n",
    "try:\n",
    "    AB_join.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping all \")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "finally:\n",
    "    AB_join.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7fece8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Violation count AFTER stream: 50107\n",
      "New records added: 73\n"
     ]
    }
   ],
   "source": [
    "# testing \n",
    "after_count = violation_coll.count_documents({})\n",
    "print(f\"Violation count AFTER stream: {after_count}\")\n",
    "print(f\"New records added: {after_count - before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "a1a13de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_ab_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06870e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_bc_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0fe1df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ./chk_violation_single_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e19305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AB Violation Stream Writer \n",
    "AB_join = AB_join.writeStream \\\n",
    "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 1)) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_ab_writer\") \\\n",
    "    .start()\n",
    "\n",
    "# BC Violation Stream Writer\n",
    "BC_join = BC_join.writeStream \\\n",
    "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 1)) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_bc_writer\") \\\n",
    "    .start()\n",
    "\n",
    "# Single-point Violation Stream Writer\n",
    "single_join = violations_union.writeStream \\\n",
    "    .foreachBatch(lambda df, batch_id: write_union_batch_to_mongo(df, batch_id, 2)) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"./chk_violation_single_writer\") \\\n",
    "    .start()\n",
    "\n",
    "# Terminate All Streams\n",
    "try:\n",
    "    AB_join.awaitTermination()\n",
    "    BC_join.awaitTermination()\n",
    "    single_join.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\" Interrupted by user. Stopping all streams.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "finally:\n",
    "    AB_join.stop()\n",
    "    BC_join.stop()\n",
    "    single_join.stop()\n",
    "    print(\"All writers stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11c4a",
   "metadata": {},
   "source": [
    "### Task 2.2 Data Visualisation\n",
    "\n",
    "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s), \n",
    "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values. \n",
    "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
    "\n",
    "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
    "* number of violations between the checkpoints\n",
    "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
    "\n",
    "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
    "\n",
    "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b82f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
    "\n",
    "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29b622",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
    "\n",
    "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
    "\n",
    "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
    "\n",
    "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
